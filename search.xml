<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Plan C</title>
    <url>/2021/04/03/Plan-C/</url>
    <content><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="就这？" data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <script id="hbeData" type="hbeData" data-hmacdigest="2e5f1b1bf7e2abc8029c2f36891e89ac226db3a55289fdce3ec1111a32362671">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</script>
  <div class="hbe hbe-content">
    <div class="hbe hbe-input hbe-input-default">
      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">
      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">
        <span class="hbe hbe-input-label-content hbe-input-label-content-default">猜对算我输</span>
      </label>
    </div>
  </div>
</div>
<script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>Classified</category>
      </categories>
      <tags>
        <tag>Classified</tag>
      </tags>
  </entry>
  <entry>
    <title>Convolution Ch 1</title>
    <url>/2021/09/09/Convolution-Ch-1/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h1 id="Convolution-Chapter-1"><a href="#Convolution-Chapter-1" class="headerlink" title="Convolution Chapter 1"></a>Convolution Chapter 1</h1><br>

<h2 id="1-1-Edge-Detection"><a href="#1-1-Edge-Detection" class="headerlink" title="1.1 Edge Detection"></a>1.1 Edge Detection</h2><h3 id="Padding"><a href="#Padding" class="headerlink" title="Padding"></a>Padding</h3><p>p: border width</p>
<p>$p = \frac{f - 1}{2}$</p>
<ul>
<li><p>Types</p>
<ul>
<li>Valid: No padding.</li>
<li>Same: Pad so  that output size is the same as the input size</li>
</ul>
</li>
<li><p>Tips</p>
<ul>
<li>Use odd number filters: Has a central pixel like a distinguisher. It’s a convention</li>
</ul>
</li>
</ul>
<h3 id="Strided-convolution"><a href="#Strided-convolution" class="headerlink" title="Strided convolution"></a>Strided convolution</h3><ul>
<li>Concept</li>
</ul>
<p>Move the filter by more steps.</p>
<ul>
<li>Formula</li>
</ul>
<p>Padding - p, Stride - s, n x n * f x f.</p>
<ul>
<li>Filtered = $\lfloor \frac{n + 2p - f}{s} + 1\rfloor$x$\lfloor \frac{n + 2p - f}{s} + 1\rfloor$</li>
</ul>
<h3 id="Tips"><a href="#Tips" class="headerlink" title="Tips"></a>Tips</h3><p>The way this is implemented is that you take this type of blue box multiplication only if the blue box is fully contained within the image or the image plus to the padding and if any of this blue box kind of part of it hangs outside and you just do not do that computation.</p>
<h3 id="Differences-to-Convolution-in-math-text-book"><a href="#Differences-to-Convolution-in-math-text-book" class="headerlink" title="Differences to Convolution in math text book"></a>Differences to Convolution in math text book</h3><ul>
<li>Concept</li>
</ul>
<p>In math text book, we will turn the filter on the horizontal as well as vertical. It will have a good property: associativity. But in neural net work area, it is not important.</p>
<p><img src="/2021/09/09/Convolution-Ch-1/1.png" alt="image-20210905145347426"></p>
<br>

<h2 id="1-2-Convolutions-over-Volume"><a href="#1-2-Convolutions-over-Volume" class="headerlink" title="1.2 Convolutions over Volume"></a>1.2 Convolutions over Volume</h2><h3 id="Convolutions-on-RGB-image"><a href="#Convolutions-on-RGB-image" class="headerlink" title="Convolutions on RGB image"></a>Convolutions on RGB image</h3><img src="/2021/09/09/Convolution-Ch-1/2.png" alt="image-20210905145952650" style="zoom:50%;">

<h3 id="Multiple-filters"><a href="#Multiple-filters" class="headerlink" title="Multiple filters"></a>Multiple filters</h3><img src="/2021/09/09/Convolution-Ch-1/3.png" alt="image-20210905151545348" style="zoom:50%;">



<br>

<h2 id="1-3-One-Layer-of-a-Convolution-Network"><a href="#1-3-One-Layer-of-a-Convolution-Network" class="headerlink" title="1.3 One Layer of a Convolution Network"></a>1.3 One Layer of a Convolution Network</h2><h3 id="Concept"><a href="#Concept" class="headerlink" title="Concept"></a>Concept</h3><p>Apply the <code>bias</code> and <code>non-linearity</code> in the end.</p>
<img src="/2021/09/09/Convolution-Ch-1/4.png" alt="image-20210905155902817" style="zoom:50%;">

<ul>
<li><p>Filter is actually the <code>w</code> in z = w * a + b.</p>
</li>
<li><p>a = g(z), g is the <code>non-linearity</code></p>
</li>
<li><p>Input and output(And some notations)</p>
<img src="/2021/09/09/Convolution-Ch-1/5.png" alt="image-20210906155903121" style="zoom:50%;"></li>
</ul>
<h3 id="Tips-1"><a href="#Tips-1" class="headerlink" title="Tips"></a>Tips</h3><p>If the filter works, it works no matter how big the image is.</p>
<h3 id="Types-of-layers-in-a-convolutional-network"><a href="#Types-of-layers-in-a-convolutional-network" class="headerlink" title="Types of layers in a convolutional network"></a>Types of layers in a convolutional network</h3><ul>
<li>Convlolution (Conv) - so far we have learned</li>
<li>Pooling (Pool)</li>
<li>Fully connected (FC)</li>
</ul>
<br>

<h2 id="1-4-Pooling-Layers"><a href="#1-4-Pooling-Layers" class="headerlink" title="1.4 Pooling Layers"></a>1.4 Pooling Layers</h2><ul>
<li><p>Concept: To reduce the size of the representation to speed the computation as well as make some of the features that detects a bit more robust.</p>
</li>
<li><p>Types</p>
<ul>
<li><p>Max Pooling</p>
<img src="/2021/09/09/Convolution-Ch-1/6.png" alt="image-20210907114129759" style="zoom: 67%;">

<p>It’s like f = 2, s = 2(filter size, stride)</p>
<p>Intuition: We take the original area as the set of certain feature, then a big number may indicate that the layer has detected a specific feature.</p>
<p>Real reason: <strong>Works well at ConvNet</strong></p>
</li>
<li><p>Average Pooling</p>
<img src="/2021/09/09/Convolution-Ch-1/7.png" alt="image-20210907115604512" style="zoom: 67%;">

<p>Sometimes used in a very deep NN.</p>
</li>
</ul>
</li>
<li><p>Summary:</p>
<ul>
<li>Hyper parameters: f -&gt; filter size, s -&gt; stride (no padding)</li>
<li>No parameters for learning</li>
</ul>
</li>
</ul>
<br>

<h2 id="1-5-CNN-Example"><a href="#1-5-CNN-Example" class="headerlink" title="1.5 CNN Example"></a>1.5 CNN Example</h2><ul>
<li><p>NN example(LeNet5)</p>
<p>Conventions:</p>
<ul>
<li>1: Call the Conv layer and Pooling layer respectively a layer.(Here Andrew uses this kind of convention)</li>
<li>2: Call them together a layer.</li>
</ul>
<p><img src="/2021/09/09/Convolution-Ch-1/8.png" alt="image-20210907141522496"></p>
</li>
</ul>
<p> From above, we can see a classic network that has Conv, Pooling, Fully Connected and Softmax.</p>
<ul>
<li><p>The parameters of the example</p>
<img src="/2021/09/09/Convolution-Ch-1/9.png" alt="image-20210907141806812" style="zoom:67%;">

<ul>
<li>Pooling layer has no parameters</li>
<li>Conv layer has few parameters</li>
<li>FC layer has many parameters</li>
</ul>
</li>
</ul>
<br>

<h2 id="1-6-Why-Convolutions"><a href="#1-6-Why-Convolutions" class="headerlink" title="1.6 Why Convolutions"></a>1.6 Why Convolutions</h2><ul>
<li><p>Two advantages</p>
<ul>
<li>Parameters sharing</li>
<li>Sparsity of connections</li>
</ul>
</li>
<li><p>Example</p>
<ul>
<li>To connect 3072(32 x 32 x 3) units and 4704(28 x 28 x 6) units, Conv layer use the filters that the shape is f = 5 and we have 6 filters. The number is = (5 * 5 * 3 + 1)  * 6 = 456, while the FC has 3072 x 4704 parameters.</li>
</ul>
</li>
<li><p>Reasons</p>
<ul>
<li><strong>Parameter sharing: A feature detector (such as a vertical edge detector) that’s useful in one part of the image is probably useful in another part of the image.</strong></li>
<li><strong>Sparsity of connections: In each layer, each output value depends only a small number of inputs.</strong></li>
<li>Images: When we shift the image for few pixels, it often retains the same features. The Conv layer can help the NN to learn this kind of property.</li>
</ul>
</li>
</ul>]]></content>
      <categories>
        <category>Research</category>
        <category>Learning</category>
        <category>Notes</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title>写在二十岁：Just a Feeling</title>
    <url>/2021/08/11/%E5%86%99%E5%9C%A8%E4%BA%8C%E5%8D%81%E5%B2%81/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>&emsp;&emsp;据说一个人的QQ名的变化能看出来一个人的成长。</p>
<p>&emsp;&emsp;于是我想了想我的：（小学）奥特曼 – 星语星愿 – 乔克先生 ，（初中） 罂粟之花 – 毛主席夸我帅 – 聚半仙， （高中） Infinity – Happily – Mercer ，（大学） DAYDREAM – SuperM。也许还有几个，但想不起来了。取这些名字的原因我都还记得：奥特曼是因为小时候喜欢奥特曼，星语星愿是因为小时候练吉他常弹而觉得好听的曲目，乔克先生是因为喜欢周杰伦。罂粟之花是觉得罂粟有毒而美丽很酷，毛主席夸我帅是因为觉得很逗比，聚半仙是喜欢看的电视剧《正阳门下》里茶楼的名字。Infinity是因为喜欢OneDirection，Happily是在自己沉郁的时期，希望自己能开心起来，Mercer（我的英文名）是希望BeMyself。DAYDREAM是因为喜欢做白日梦，SuperM是希望自己能变得厉害。。。</p>
<p>&emsp;&emsp;如数家珍，但也许也忘了些。</p>
<p>&emsp;&emsp;想到过去，突然就不困了，也慢慢明白是什么奇怪的感觉驱使着我在写着这些流水账，在二十岁的今天：</p>
<p>&emsp;&emsp;再不写下来，我就要忘了。</p>
<p>&emsp;&emsp;可我不想忘记。</p>
<p>&emsp;&emsp;把过去写给自己。</p>
<br>

<h2 id="胆小鬼"><a href="#胆小鬼" class="headerlink" title="胆小鬼"></a>胆小鬼</h2><p>&emsp;&emsp;小时候真的很怕死。假设自己能活一百岁，往往想着：再过xx年我就死了啊，唉。</p>
<p>&emsp;&emsp;记得有一次躺在老家的床上，睡不着，躺在床上，透过窗户望着对面的楼，有几扇窗户依旧灯火通明，而其他的都暗了。我在床上一动也不动，那栋楼一动也不动，好像整个世界一动也不动。空气凝固般的安静。睡不着，却也不知道该想些什么，整个世界也安静着，没有任何反馈——我就突然想到了死，因为我小时候理解的死就是：安静的发慌，世界仿佛停止转动，你也不知道去向何方。然而这样的安静在这样的深夜里持续着，让我感到越来越窒息，于是终于——哭了，大哭特哭。后来我妈被吵醒了，把我抱走，跟她睡。</p>
<p>&emsp;&emsp;第二天早上醒来，世界清亮，眼前满目色彩，耳边清晨鸟鸣，世界运转，昨晚的烦恼好像没发生过。</p>
<p>&emsp;&emsp;当然后来的晚上，还是哭了好几次。有次一大家子人在外婆家睡觉，也这样”犯病“了，吵得整个外婆家睡不安宁，于是舅舅从床上穿好衣服，连夜把我送回了我家，让我妈抱着睡。</p>
<p>&emsp;&emsp;后来怕死被怕鬼代替了。</p>
<p>&emsp;&emsp;其实我没看过一部鬼片，顶天了就是港片开心鬼的水平。但是我是看迪迦奥特曼都能被吓到沙发底下去的主儿，所以经常性怕鬼。有一次（应该不到八岁）在成都神仙树的一家餐厅吃饭，餐厅送给我一个儿童的戴在手上的啪啪圈，上面印着一个海豚。不久后某天的晚上我梦见，一个女人打开了一扇大门，满脸笑意的朝我走来，手上拿着一个小小的海豚玩偶，朝我越来越近。突然，那个女的停了下来，笑容凝固，把海豚捏在手里举起来放在左眼前，”砰“的一下就把它按进了眼睛里面。</p>
<p>&emsp;&emsp;吓得我一声尖叫猛然惊醒，穿上拖鞋就往卧室外啪嗒啪嗒跑，发现家里人正在吃早饭，转过头来一脸懵逼的看着我。我说你们咋不叫我呢，妈说想着让我再睡会儿。于是我心有余悸的爬上了座位，开始扒拉起了早饭。</p>
<p>&emsp;&emsp;后来妈为了锻炼我的自主能力开始让我一个人睡觉。一开始必须要开灯开门，然后把被子蒙上睡觉，隔一会儿觉得闷了，就出来换口气，后来学着把被子打开一个小口呼吸。慢慢地折腾个一小时，就睡着了。</p>
<p>&emsp;&emsp;因为怕鬼闹过不少好笑的事，什么夏天洗澡也要取暖全开亮度最大，什么趴在姐姐床前求她和我一起睡惨遭拒绝。。。</p>
<p>&emsp;&emsp;其实还怕很多事。</p>
<p>&emsp;&emsp;很小的时候喜欢一个女生，某天在家里上网做与学习相关的事的时候，悄悄登录了QQ，突然发现那个女生给我发了好友申请。那应该是我这二十年来最激动的好友申请，刚打开准备点确认，我妈却突然把门打开了进来拿点东西，吓得我鼠标一滑点到了拒绝上，然后迅速关掉了QQ。等妈出去了之后，看着挂掉的好友申请，气的敲桌子，追悔莫及。</p>
<p>&emsp;&emsp;当然后来我向她一番拐弯抹角的解释，还是加上了。</p>
<p>&emsp;&emsp;有一次大家约着出去玩，那个女生也在。我一大早的坐了公交车到万达广场等着，等了半个小时到点了没人来，就又等了会儿。后来才被告知，那天另外某个同学有事不来了，于是大家干脆都不来了。因为我出发的早而且没手机，都是我找人借手机打电话给我妈，我妈和我说的。于是我又一个人坐公交车回了家。</p>
<p>&emsp;&emsp;其实还做了挺多事，比如强行把选修课改成和她一样的书法。班上就我和她选了书法，于是合情合理的做了在书法班上做了同桌。她问我为啥也选书法，我叹了口气说：”科学班满了把我踢走啦，安排在这。不然我才不想来学这破书法。“比如她说她家楼下没有超市，让我帮着买当时最好喝的真果粒。于是周末回家，我跑到小区超市里，每个口味的真果粒都选了几瓶，装在一个黑色的垃圾袋里，周一上学的时候，放在了她的抽屉里。</p>
<p>&emsp;&emsp;后来我想，当时怎么会蠢到用垃圾袋装呢？再后来我想，当时怎么会蠢到不直接买箱装的真果粒呢？再再后来的我想，她家住的那个高档小区里怎么可能没超市卖真果粒呢？</p>
<p>&emsp;&emsp;有次几个家长约着家长和孩子去旅行，包括我家和她家。那个时候已经毕业各奔东西了，我怕再次遇见她的时候，我还是个小矮个，于是在寝室的每天晚上开始练仰卧起坐和俯卧撑，一个晚上没有也没停过。</p>
<p>&emsp;&emsp;半年后，旅行取消了。目的地发生了暴乱，家长们担心危险，就说算了。</p>
<p>&emsp;&emsp;不过我也并没有长高，因为那时的我不明白仰卧起坐和俯卧撑不是增高的运动，哈哈。</p>
<p>&emsp;&emsp;无论做了多少事，其实都没有表白过，一直压在心底。也许是自卑, 怕我的小矮个，怕我长得不帅，怕被拒绝，就这样怕着，直到我和她慢慢淡出彼此的世界。</p>
<br>

<h2 id="娱乐至上"><a href="#娱乐至上" class="headerlink" title="娱乐至上"></a>娱乐至上</h2><p>&emsp;&emsp;小的时候爱玩，一开始都是跟着别人玩。跟着小伙伴玩弹珠，游戏王，陀螺。。。跟着奶奶翻来覆去的看新白娘子传奇，还珠格格，排球女将。后来开始跟着姐姐玩，玩摩尔庄园。后来小学里不流行摩尔庄园了，开始流行赛尔号，4399如狂扁小朋友，后来又是功夫派。。。流行什么玩什么。现在觉得还是摩尔庄园好玩，零花钱大作战和黑森林到现在都还记得清楚。</p>
<p>&emsp;&emsp;那个时候还跟着姐姐追星。那个时候大街上还有报亭，卖《星周刊》，专门讲娱乐圈的八卦新闻，姐姐每周都买，她那会儿喜欢张杰。我印象深刻的新闻是周杰伦花xx钱买了个定制的蝙蝠车，苏打绿演唱会只唱了四（反正很少？）首歌歌迷不满要求退票，后来又是俞灏明Selina烧伤事件登上首版。。。那会儿明星很多，像黑Girl，棒棒糖，By2，南拳妈妈，至上励合等都还很火。</p>
<p>&emsp;&emsp;家里电视就那一个，看的剧也和家里人一样。《又见一帘幽梦》《情深深雨蒙蒙》《爱情魔发师》《微笑Pasta》《绿光森林》《恶作剧之吻》《公主小妹》《放羊的星星》。。。太多了，印象最深刻的应该是《微笑Pasta》，看了得有四五遍。后来跟着姐姐追韩剧，从《城市猎人》到《屋塔房王世子》再到《听见你的声音》，到后来《想你》什么的都看过了。以至于《来自星星的你》在中国大火的时候大概是在十集更新的时候，然而我跟着老姐开始看的时候才更新到第六集。</p>
<p>&emsp;&emsp;初中的时候爱玩刺客信条，这是中二的起源。跟着几个死党在校园里装作刺客鬼鬼祟祟地乱窜。后来寝室一楼装了连着一排的防盗窗，于是晚上等生活老师停止巡逻，我们几个住二楼地就从窗户翻出寝室，踩着二楼的排风箱，再踩在一楼的防盗窗上，就可以顺着防盗窗行走了。在夜里，活像个刺客。还可以顺着防盗窗走到二楼其他寝室的窗户外，踩着他们外部的排风箱，翻进窗户进入寝室，吓别人一跳。这样，走廊里的生活老师就不会发现我们钻到其他寝室去了。</p>
<p>&emsp;&emsp;后来我和某个女生聊起此事，我开始吹嘘起来，并且说，我们还可以从一楼的防盗窗上爬下来，这样我们就算离开寝室楼也不会被老师发现啦。那个女生眨巴眨巴眼睛说：那你可以这样就跑到女生寝室楼来找我玩了！后来想起，她是不是有别的意思？也许只是戏谑我罢了。</p>
<p>&emsp;&emsp;然而当时的我并没有多想，总之，万物皆虚，万物皆允（刺客信条名言），我是一名刺客，over。</p>
<p>&emsp;&emsp;但是现在，不再有以前那么多样的娱乐，日常不再那么好玩了。取而代之的是不断地忙碌与不断地抉择，和不断地离别。</p>
<br>

<h2 id="兄弟"><a href="#兄弟" class="headerlink" title="兄弟"></a>兄弟</h2><p>&emsp;&emsp;从小到大其实有过很多兄弟。</p>
<p>&emsp;&emsp;幼儿园的兄弟都是打架的兄弟，每次见面要先过两招。打架厉害了，别人自然也就对你心生敬佩。打架也是男人的人生第一课，既让你明白了拳头硬才是硬道理，也让你明白了，有时候逞强，就会被打的很惨。到了小学，打架和游戏五五开，一半时间在打游戏，一半时间在打架。有一个人特别厉害，我现在还记得他一个人在操场上打七八个，狂捶着自己的胸脯说”你们来啊！“。仿佛是打不坏的金刚，颇有吕布的气势。碰巧的是，他和我后来在一个高中，不过他是体育足球生。在高中和他再次相遇的时候，不再是当年吕布狂放不羁的模样，而是冷峻酷酷的面容。</p>
<p>&emsp;&emsp;打架我是打不了的，太瘦弱了，所以我和几个兄弟转行打游戏。小学那会儿，放学跑到对面的小卖部买几包辣条，就跑到别人家里一起打游戏了。有时候也不打游戏，就一起去溜冰，在小区里窜来窜去。每天放学后聚到一个人的家里玩赛尔号，一个人操作，集体出谋划策，说这个BOSS应该怎么怎么通关。。。现在还记得有次打赛尔号里有个BOSS叫魔狮迪露，一帮人围在电脑前，紧张地盯着屏幕，希望魔焰猩猩那有3%几率秒杀对手的绝命火焰可以秒杀BOSS。那天下午打了得有好几十把吧，才通关了。后来我觊觎我兄弟练好的一个极品号，于是把他的号偷了，和他闹了一次矛盾。2021年和远在美国的他和我聊起此事，说当年你偷了我赛尔号的号，你还记得吗？我只能说我还记得，怪不好意思的。也没明白，当时为啥会干这么龌龊的事呢？我向他解释道，年少的我经不起诱惑。他哈哈笑道，没事，早就过去了。</p>
<p>&emsp;&emsp;小学到初中都是游戏少年，大家吃饭聊游戏，下课聊游戏。小学到初中，无非是从赛尔号这种网页游戏进化到了刺客信条这种单机游戏罢了。</p>
<p>&emsp;&emsp;到了高中，开始变老变宅，看起了番（日本动漫）。说实话，我刚上高中的时候是不看番的。但刚搬进高中新寝室的时候，里面C、O、G正在热火朝天地讨论着最近的新番。G给我打招呼，我注意到，他身上的衣服画着一个大大的女性动漫角色。他的床上，还有一个动漫抱枕。后来，学校要求寝室装饰的时候，他们买了几张动漫的壁纸，贴在了寝室的瓷砖墙上，一个一个给我指认：这个是时崎狂三，这个是Balabala。。。</p>
<p>&emsp;&emsp;我才明白，这下是遇见几个老司机，进”贼窝“了。所以，后来的我开始看番，也是”理所应当“。</p>
<p>&emsp;&emsp;C是个资深中二二刺螈，他有一个日记本，记着一些他喜欢的日漫经典台词。当然，也会记一些我们的日常。他把我在旁边一脸讥笑地看他写日记的模样，也画进了日记里。其实就是一个简笔画的圆脸，两个弯弯当眼睛，然后把笑起来的嘴巴夸张的画的特别大，特别邪恶。哥几个看了都说像，我自己看了也觉得。我经常会朗诵他日记里的台词，比如”玫瑰色的人生“（指恋爱）。我经常说，C啊，你的玫瑰色人生什么时候才到来呢？不过这是我拿来讥讽他的台词。我印象最深刻的是他难得一本正经的给我介绍：</p>
<p>&emsp;&emsp;”Ice cream“。</p>
<p>&emsp;&emsp;”什么意思？“我问。他和我说，这是《冰菓》（一部有关青春的）里的台词，他特别喜欢。Ice cream的读音其实有特别的含义，Ice cream = I scream，冰菓 = 我在呐喊。其实那个时候我还没看过冰菓，但是我一下就记住了，我们打完羽毛球时，他难得收起嬉皮笑脸，认真的给我解释的这个台词。scream，呐喊，和青春结合起来，总是能给人以丰富的联想。</p>
<p>&emsp;&emsp;高一结束了，身边的兄弟不再只有C、O、G，人越来越多。我们感慨，高一就这么过去了，一切感觉平平无奇。没有什么”玫瑰色的人生“，生活好像是灰色的，事情都按部就班的进行着。我说：一点都没有青春该有的样子。</p>
<p>&emsp;&emsp;那时感慨青春无趣的我们，并没有想到未来的两年会发生那么多笑泪并存的事，经历了一次一次，属于青春的，”scream“。</p>
<br>

<p>&emsp;&emsp;高中毕业后，我们各奔东西，但依然保持着高度的联系。偶尔聊起高中，都会陷入一些回忆里。其实本来还想写点骚话，但怕他们看到，下次见面被他们”安排“。我想我以后会把这些回忆，这些”骚话“，写在一些有趣的文章里的。</p>
<p>&emsp;&emsp;属于我们的路还很长。</p>
<br>

<h2 id="失败"><a href="#失败" class="headerlink" title="失败"></a>失败</h2><p>&emsp;&emsp;曾经有个和我合不来的班主任。在我最叛逆的时候，她把我叫出来，狠狠地训了一顿。我眼睛朝着地下，嘴角带着无所谓的笑。她叹了一口气，说：”xxx啊xxx，你就是过得太顺了。“</p>
<p>&emsp;&emsp;我把笑容收了起来。</p>
<p>&emsp;&emsp;你又明白我什么呢？</p>
<br>

<p>&emsp;&emsp;学生时代的失败往往都是和学习与考试挂钩。</p>
<p>&emsp;&emsp;小学三年级结束之后，家里为了住校方便，把我转去了另外一个私立学校。三年级结束的时候，那会儿成绩还是很不错的，但是我并没有通过那所私立学校的入校考试。我现在还记得，考语文的时候，语文分北师大版和人教版，让我们根据自己在以前学校学的版类，去拿对应的卷子。只有我糊里糊涂地，学的北师大拿的人教版，做到一半才觉得不对劲，把卷子拿上去问老师。老师赶紧让我换了卷子重新做，而等考试结束铃响的时候，我那份卷子才做了一半。考完之后等待结果，妈妈发现我榜上无名，只好牵着我的手回了家。回家的路上，我一边哭一边抹眼泪，觉得自己真的好瓜。</p>
<p>&emsp;&emsp;后来还是进了那所学校，是妈妈找人托了关系。转校之后的我倒是过的顺风顺水，成绩也很不错。但五年级的时候，班主任有一次因为某事训斥全班，突然提到：”不要以为我不知道，你们某些人是通过关系进来的！其实我花名册上都记的清清楚楚！“。听到那句话的时候，像是被打回原形一样，不敢看着台上的班主任。就像小时候动画片里的经典桥段，一个冒名顶替的人，终被别人指认成大骗子，冒牌货。</p>
<br>

<p>&emsp;&emsp;小升初的时候，参加了成都好几所学校的考试。那个时候成绩已经是班上名列前茅，考试的时候还是非常有信心。但是成绩出来之后，好的学校，我都没有考上。考当时一所很有名的私立的时候，班上好几个成绩只是中等偏上的同学都考上了，但我没有。我数学考的奇低无比，班主任知道了，都疑惑着摇摇头。考另外一所学校的时候，整个人心态其实已经有点崩了。考试的过程更是摧残自己的信心，好多道题没做出来。考完也知道自己凉了。好不容易，终于通过了一所很好的公立学校的面试，又重燃起了希望。然而第二轮是摇号，运气不好，没摇到。</p>
<p>&emsp;&emsp;最后吃了所在私立小学的福利，升上了直系的私立初中。</p>
<br>

<p>&emsp;&emsp;初升高的时候是经典复刻。同样的是班上男生成绩最好，非常有希望考上市里最好的学校。同样的，并没有考上。成绩出来了，家里都挺沉默，爸爸叫了辆出租车，带着我去各个学校打听收分线的情况。他在前座不断地打着电话，焦急的问着问那。我在后座，一个人什么也做不了，也不知道做些什么。靠着车窗，看着窗外不断流失的街景，只觉得自己无能为力，软弱不堪。</p>
<p>&emsp;&emsp;最终去了一所学校的分校。跑到那所学校里填名字的时候，突然碰到了很久以前的玩伴。他一脸惊喜的看到我，说”你也上这所学校啊？“，还看了下我的分数：欸，一模一样，好巧！那时的我心里很不是滋味。因为我知道，我的成绩其实是比他要好的。我和他没说几句，填完信息，就赶紧回家了。</p>
<p>&emsp;&emsp;后来我和他分到了一个班，甚至一个寝室，又重新成为了很好的朋友。我有次和他坦诚地说，其实当时在填学校的时候，看见你，让我觉得有点耻辱。然后马上又为从前的自己向他道歉。他摆摆手，说没事，反正都过去了。那时看见我，他也挺意外的。</p>
<p>&emsp;&emsp;其实他就是G。</p>
<br>

<p>&emsp;&emsp;高中的时候，选择了一条和别人不一样的路。我参加了信息学竞赛，在NOIP上获得了不错的成绩，并且被教练选中留下来冲刺省队。幸运的是，NOIP的成绩还让我进入了北大的冬令营，有希望签署降分的协议。在那个大雪纷飞的长沙，我又一次考的一塌糊涂。冬令营的题全考到了我最薄弱的期望概率部分，最后考的连100分都不到，拿到了最差的协议。同行的另外一名同学，叫他K吧，考的非常好，考完后和我眉飞色舞的说着解法，而我无心应付，闷着头赶路。</p>
<p>&emsp;&emsp;离开长沙的时候，教练组织着一起吃顿”庆功宴“。菜慢慢地上了一大桌，饿坏了的我刚想夹菜，教练却突然收到了电话，说K有希望换到更好的协议。于是教练带着K和他的家人，火急火燎的离开了餐厅，去找北大的老师理论了。</p>
<p>&emsp;&emsp;于是整个餐桌，只剩下我这一家。</p>
<p>&emsp;&emsp;我知道，在教练他们回来之前，我不能动筷。便盯着桌上的菜，在冬日里冒着升腾的热气，砂锅的盖扑腾扑腾地响，汤咕噜咕噜地翻滚着。我静静的坐在椅子上，什么也做不了，也不知道做些什么。</p>
<p>&emsp;&emsp;为什么总是这么软弱不堪呢。</p>
<p>&emsp;&emsp;和爸妈订了隔晚的机票，飞回了成都。在飞机上，妈妈突然语重心长的给我讲：”儿子，其实考差了没关系，但我希望你能快乐一点。你看K啊，天天都很有朝气，这才是十六七岁应该有的样子啊！不要老是这样瘫着，无精打采的。振作起来，好吗？“ 我把脸偏向另外一侧，看着窗外又大又圆的月亮，想起了昨晚正是”超级月亮“的夜晚。我想想点别的，但又想起了妈提到的K，想起他眉飞色舞和我说着解法的样子，又想起总是在关键时刻掉链子的我。</p>
<p>&emsp;&emsp;我只能沉默，不知道怎么回应她。</p>
<br>

<p>&emsp;&emsp;高二的最后，省选也失利了，停课了一年的竞赛画上句点，我滚回了原来的班级，学习文化课，准备高考。回到班级的时候，像是白手起家。已经阔别班级一年，班上有些人看着我，都觉得有些陌生。此时，我已经有一年没上课了，但我只剩下一年了。用两年去追逐别人的三年，每每想到，我都会觉得很绝望。开始害怕，自己和从前一样，在重要的考试上，考得一塌糊涂。</p>
<p>&emsp;&emsp;像是被逼到了绝境，没有退路，只有开始日复一日的努力。尽管过程中有着意外与坎坷，但我总算在高考考出了一个还不错的成绩，裸分考上了一所很好的学校，打破了这么多年来的”魔咒“。</p>
<br>

<p>&emsp;&emsp;其实二十年来的失败不只是学习，其实别人的失败，或许比我还要多的多。没有人总是”太顺“。一次成功，往往要经历很多次刻骨铭心的失败。我想说，我很谢谢这么多年来的这么多失败，让我不再是当年那个边走边摸眼泪的小孩，不再是那个靠着出租车车窗眼神迷茫的小孩，不再是背对着母亲不敢回应她的小孩。</p>
<p>&emsp;&emsp;不再那么软弱不堪，我已经是能勇敢迎接未来的每次失败的“大人”了。</p>
<h2 id="Feeling"><a href="#Feeling" class="headerlink" title="Feeling"></a>Feeling</h2><p>&emsp;&emsp;以前朋友羡慕我记忆力好，背课文很快，隔得很远的事情，也能记得。我也觉得这是一种幸运，但随着长大，又渐渐改变了想法。过去记得太多，有的时候，慢慢成了现在的牵绊，积蓄了惆怅。即使和别人激动地说起某段回忆，TA会笑着说：“啊！还有这事，哈哈，我都忘了。”这个时候，会有一种期待落空的感觉，又怀疑是不是自己凭空捏造的妄想。</p>
<p>&emsp;&emsp;到了二十岁的今天，我的记忆力不再那么好了，我反而变得轻松。但仍有一种奇妙的感觉驱使着我，就像它驱使着我写下这篇流水账一样，让我记住过去的一切。感觉告诉我，不要忘记过去。于是我把从小到大，别人送给我，有着特殊的意义的，信件贺卡、礼物，装在了一个袋子里。这个袋子是我高中毕业旅行，在东京三鹰之森买的宫崎骏纪念袋。以后每每感到迷惘的时候，就把袋子打开，一封一封，一样一样地，慢慢地阅读，阅读着别人送给我时的心情，阅读我在他们眼里的模样，阅读那些镀金的时光。</p>
<p>&emsp;&emsp;于是，渐渐想起自己是一个怎样的人，自己想成为一个怎样的人，渐渐想起，不断在我人生关键时刻，涌入脑海的那个奇妙的“感觉”。</p>
<br>

<p>&emsp;&emsp;这种感觉，像是命运，像是神的低语。人在一生中，总会有那么几个瞬间，仿佛是命中注定一般，做出和这个星球上其他八十亿人都不会做的事，从而让你和别人区分开来，在别人的眼中是那么的不同。</p>
<p>&emsp;&emsp;在这样的瞬间，心情是紧张的，脸颊是涨红的。但“感觉”出现了，我仿佛获得了一股不曾有但本该属于我的力量，勇敢地去做了别人都不会做的事。等一切结束落幕，我回头看着围观的人们的时候，他们的眼神是惊奇的，仿佛看着外来生物一样的看着你。有的人钦佩，也有的人讥讽。但没关系，我做了，就够了。</p>
<p>&emsp;&emsp;在“感觉”的驱使之下，我做了很多“不一样的事”：在艺术节上申请独自登台唱一首歌；给出一道题和班上所有人都不同的解答；向周围所有人大声的说出我的想法，哪怕他不是主流的；在大家都选择沉默的时候，扎破和平的气球；在大家按部就班的时候，选择一条和别人不一样的路，追逐着自己的梦想。。。我有我的理由，我喜欢这种“感觉”，喜欢这个Feeling。因为那个时候，我觉得我是在发光的。</p>
<p>&emsp;&emsp;阅读着别人送给我的文字，有的时候就能透过纸张与相框，看到过去，那个在别人眼里发光的自己，自己也喜欢的自己。希望Feeling能一直伴随着我，在未来，去经历泪水与感动，做着一件件，印着“自我”的符号的“大事”。</p>
<br>

<p>&emsp;&emsp;十七岁的我在高中即将毕业的时候，许愿未来做一个温柔的人。现在我许愿做一个不急不慢的人。不急不慢地睁开眼，不急不慢地洗漱，不急不慢地骑着自行车去上课，不急不慢地在超市里兜圈，不急不慢地旅行，不急不慢地准备着未来。成长的纷扰已经越来越多，我希望我能在未来踏踏实实，不急不慢地走好每一步。</p>
<p>&emsp;&emsp;二十岁也将过去，我会迎来我的二十一岁，二十二岁，二十三岁。。。一切都将成为过去，但我想我不能忘记。它一直在我身后，注视着我，期待着，我能成为自己想要成为的人。</p>
<p>&emsp;&emsp;最后，祝自己二十岁生日快乐。</p>]]></content>
      <categories>
        <category>Life</category>
        <category>Summary</category>
      </categories>
      <tags>
        <tag>Summary</tag>
        <tag>Nostalgia</tag>
      </tags>
  </entry>
  <entry>
    <title>CS224W Sec 6~7</title>
    <url>/2021/06/11/CS224W-Sec-6-7/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h1 id="CS224W-Sec-6-7"><a href="#CS224W-Sec-6-7" class="headerlink" title="CS224W Sec 6 ~ 7"></a>CS224W Sec 6 ~ 7</h1><h2 id="Section-6"><a href="#Section-6" class="headerlink" title="Section 6"></a>Section 6</h2><h3 id="6-1-Intro"><a href="#6-1-Intro" class="headerlink" title="6.1 Intro"></a>6.1 Intro</h3><ul>
<li><p>Recap</p>
</li>
<li><p>Limitations of <strong>shallow embedding</strong></p>
<p><img src="/2021/06/11/CS224W-Sec-6-7/limit.png"></p>
</li>
<li><p>Deep Graph Encoders:</p>
<p>ENC(v) = multiple layers of non-linear transformations based on graph structure</p>
</li>
</ul>
<h3 id="6-2-Pipeline"><a href="#6-2-Pipeline" class="headerlink" title="6.2 Pipeline"></a>6.2 Pipeline</h3><h4 id="Basics-of-Machine-Learning"><a href="#Basics-of-Machine-Learning" class="headerlink" title="Basics of Machine Learning"></a>Basics of Machine Learning</h4><ul>
<li><p>Supervised learning: We are given input <code>x</code>, and the goal is to predict label <code>y</code></p>
<ul>
<li><p>Formulate this task as an optimization problem: Loss function</p>
<p><img src="/2021/06/11/CS224W-Sec-6-7/formulate.png"></p>
</li>
<li><p>Example: CE</p>
<p><img src="/2021/06/11/CS224W-Sec-6-7/ce.png"></p>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>How to optimize?    </p>
<p>Directional derivative!(Gradient)</p>
<p><img src="/2021/06/11/CS224W-Sec-6-7/derive.png"></p>
<p><strong>Gradient Descent</strong>:
$$
\Theta - \eta \nabla _{\Theta}\zeta \rarr \Theta
$$
$$\eta$$ is the learning rate.(Hyperparameter that controls the size of gradient step)</p>
<p><strong>Ideal termination condition</strong>: It no longer improves the performance on <strong>validation set</strong> (part of dataset we hold out from training)</p>
</li>
<li><p>Problem: <code>x</code> is the entire dataset, which means it’s <strong>expensive</strong> to calculate every gradient descent step.</p>
</li>
<li><p>Solution: Stochastic Gradient Descent(SGD)</p>
<ul>
<li><p>At every step, pick a minibatch $B$ containing a subset of the dataset, use it as input <code>x</code>. </p>
</li>
<li><p>Concepts:</p>
<ul>
<li>Batch size: The number of data points in a minibatch</li>
<li>Iteration: 1 step of SGD on a minibatch</li>
<li>Epoch: One full pass over the dataset(# iterations is equal to ratio of dataset size and batch size)</li>
</ul>
</li>
<li><p>It’s unbiased estimator of full gradient</p>
<ul>
<li>But there is no guarantee on the rate of convergence</li>
<li>In practice often requires tuning of learning of rate</li>
</ul>
</li>
<li><p>Simple Function
$$
f(x)=W * x, \Theta = {W}
$$</p>
<ul>
<li>If <code>f</code> returns a <code>scalar</code>, then W is a <em>vector</em></li>
<li>If <code>f</code> returns a <code>vector</code>, W is the weight <em>matrix</em>(Jacobian matrix) </li>
</ul>
</li>
<li><p>Back-propagation
$$
if: f(x)=W_{2}(W_{1}x), \Theta= { W_1, W_2}\
\nabla _xf=\frac{\partial f}{\partial (W_1x)}*\frac{\partial (W_1x)}{\partial x}
$$
<strong>Chain rule</strong>.</p>
</li>
<li><p> <img src="/2021/06/11/CS224W-Sec-6-7/proga.png"></p>
</li>
<li><p>Other non-linearity:</p>
<ul>
<li><p>Rectified linear unit(ReLU)
$$
ReLU(x)=max(x, 0)
$$</p>
</li>
<li><p>Sigmoid
$$
\sigma(x)=\frac{1}{1+e^{-x}}
$$</p>
</li>
</ul>
<p>Increase the <strong>expressivity</strong>.</p>
</li>
</ul>
</li>
<li><p>Multi-layer Perceptron(<strong>MLP</strong>)</p>
<ul>
<li>Each layer of MLP combines linear transformation and non-linearity
$$
x^{(l+1)}=\sigma(W_1x^{(l)}+b^l)\
\sigma: a \ non-linearity \ function (e.g.,sigmod)
$$
<img src="/2021/06/11/CS224W-Sec-6-7/instance.png"></li>
</ul>
</li>
</ul>
<h3 id="6-2-Summary"><a href="#6-2-Summary" class="headerlink" title="6.2 Summary"></a>6.2 Summary</h3><ul>
<li>objective function: $$min \ \zeta(y, f(x))$$</li>
<li>f can be a simple linear layer, an MLP or other neural networks\</li>
<li>SGD</li>
<li>Forward propagation: Given $x$, compute $\zeta$</li>
<li>Back propagation: obtain gradient $$\nabla _\Theta \zeta$$ using a <strong>chain rule</strong> </li>
</ul>
<h2 id="Words"><a href="#Words" class="headerlink" title="Words"></a>Words</h2><p>inherently:[ɪnˈhɪrəntli]: 天性地，固有地</p>
<p>transductive: 直推</p>
<p>exponentiate: 取幂</p>
<p>categorical: 绝对的</p>
<p>one-hot: 有多少个状态就有多少个比特，而且只有一个比特为1</p>
<p>derivative: 导数</p>
<p>perceptron: 感知机（模拟大脑的人工网络）</p>]]></content>
      <categories>
        <category>Research</category>
        <category>Learning</category>
        <category>Notes</category>
      </categories>
      <tags>
        <tag>Graph</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>五月天</title>
    <url>/2021/06/07/%E4%BA%94%E6%9C%88%E5%A4%A9/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><div id="aplayer-iPzOgyTX" class="aplayer aplayer-tag-marker meting-tag-marker" data-id="00134jgf28JHVf" data-server="tencent" data-type="song" data-mode="circulation" data-autoplay="false" data-mutex="true" data-listmaxheight="340px" data-preload="auto" data-theme="#ad7a86"></div>

<br>

<p>&emsp;&emsp;在老姐的歌单里面听到的这个歌，觉得挺好听的。原来听了那么多SHE的歌，居然都没发现这首歌。其实SHE挺多歌都曲风悦耳且耐听，比如你曾是少年，比如热带雨林。不过大多数印象都停留在中国话、Shreo或者波斯猫等火遍大江南北的歌了。</p>
<h2 id="生活"><a href="#生活" class="headerlink" title="生活"></a>生活</h2><p>&emsp;&emsp;杭州的天气越来越热，最热的时候也有37度了。好在平常在外只是通勤，大部分时间都呆在室内，所以感觉也还好。平常只用穿一件短袖和短裤就可以出门，手洗起来也非常方便。然而即使通勤时间不长，因为阳光曝晒和短袖的缘故，现在手臂上有一道非常明显的黑白分界线。</p>
<p>&emsp;&emsp;感觉随着年纪增长，反而越来越喜欢夏天。虽然曝晒且刺眼，但是天气晴朗，清风微拂，光照在身上，像是有能量源源不断的输送。下雨也下的很痛快，劈里啪啦。以前在成都的初春和深秋转季的时候，往往会有连续很长一段时间的阴沉天气。这个时候，阴郁的情绪往往会积蓄，像布满落叶的水凼。</p>
<p>&emsp;&emsp;大一的夏天因为疫情，就是在家里度过的。到了大二才在学校里好好感受了一下校园的夏天。穿着都很清凉，很多女生都开始穿起了JK。日本的JK校服确实很经典，很能体现校园青春感，也能展现很多女生很好的腿部线条。中国的校服就的确比较普通了，没看见过什么设计不错的校服。很多学生觉得中国校服也很好看，大多是穿久了，感情深厚，才会发现朴素的校服很有青春感。但单拎出来的话，确实比较普通，缺乏设计感。</p>
<p>&emsp;&emsp;学校开始应季的办起了很多活动，包括蛮多体育比赛。本来报名了一个趣味运动会，打算摸个鱼捞体育竞赛分，结果那天搞忘去了。只好后面再找找还有什么体育比赛，一看比赛列表：伦巴、恰恰、健美、健身。</p>
<p>&emsp;&emsp;老人，地铁，手机.jpg</p>
<p>&emsp;&emsp;后来游泳课老师给我讲六月还有一个游泳比赛，但学院没有通知。问了下费辅导员，原来是计算机学院近年来就没人参加游泳比赛，学院干脆不转通知了。费老师一听我要参加，大手一挥：好！你现在就是我们计算机学院的游泳队队长了！然后在学院群里转发了体育部的游泳通知，附上了我的QQ。</p>
<p>&emsp;&emsp;于是乎从一条咸鱼莫名变成了队长。</p>
<p>&emsp;&emsp;除此之外还报名了一个支教的项目，和一个牛津的数据分析的暑假项目。支教面试自我感觉良好，可惜没有进。牛津的项目倒是没有什么意外就进了。</p>
<p>&emsp;&emsp;学校里接连的在草坪那儿开起了音乐节。每次晚课前黄昏时，草坪聚着很多人，包括很多留学生。印象最深的是一个唱国外歌曲的乐队，主唱身型瘦削，但是嗓子很有味道，吐词也很好，唱慵懒的爵士风的乐曲很抓耳。他一边弹电吉他一边唱，表情很享受。不过他后边有一个小胖哥看起来比他更享受，身体随着节奏左摆右摆。但是他什么乐器也没拿，也没麦克风，就在乐队中央摇啊摇，头部微微上扬，面容呈享受状，就好像费玉清。</p>
<p>&emsp;&emsp;虽然看完了整场音乐会，也没搞懂小胖哥到底在乐队里面扮演的是什么角色。不过他成功让我在写日记的今天还把他给记住了。暂且把他当作是气氛组吧。</p>
<p><img src="/2021/06/07/%E4%BA%94%E6%9C%88%E5%A4%A9/concert.png"></p>
<h2 id="科研"><a href="#科研" class="headerlink" title="科研"></a>科研</h2><p>&emsp;&emsp;CS224W的讲师是一个斯坦福的副教授，但是是一个俄罗斯人。他开口的那个弹舌r音先是把我吓了一跳，然后之后基本上有r的单词都会听到他响亮的弹舌，比如“Graph”他念的是格拉夫，然后读起来是“格得了拉拉拉…夫”好像“r”在他的口中上跳下跳。B站上的课程虽说是带了中文字幕，但那个居然是机翻，Graph给翻译得是图形，严重干扰听课。于是干脆找了个只有英文字幕得听。有一说一，毛子虽然念的奇怪，但是发音清晰，而且讲课逻辑很好，通俗易懂。基本上跟听中文一样，很顺畅的就听下来了。同时Slide的质量非常高。</p>
<p>&emsp;&emsp;教授讲到举例的时候往往会先说一句”Aha！“然后再开始描述例子。长而久之，取下耳机关闭课程的时候，脑子里都是：</p>
<p>&emsp;&emsp;”Aha！“</p>
<p>&emsp;&emsp;”Aha！“</p>
<p>&emsp;&emsp;Colab还是比较难的。跟🐏老沟通了一下，打算先把理论掌握清楚，lab之后慢慢做。可能要先开始接触一下SRTP的项目了。</p>
<h2 id="实习"><a href="#实习" class="headerlink" title="实习"></a>实习</h2><p>&emsp;&emsp;微软Explore的面试没有进我是万万没想到的。笔试我大概半个小时就做完了，还检查了一遍，更不必说帮同参加笔试的同学解出了一道题。然而他进了我却没进，还是挺难受的。笔试我自我感觉很难有错，都用IDE仔细查过了。看了下知乎上很多人甚至只是普通985，笔试只做出了5道都进面试了，甚为不解。也许是微软为了追求所谓的Diversity吧，学校也许限制人数。</p>
<p>&emsp;&emsp;只有等着之前完成的实习发钱咯。这个暑假也没打算去别的地方实习了，自己水平讲真还是不太够。所以这个暑假可能除了参加牛津交流项目，剩下的时间就好好在实验室学习吧。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>&emsp;&emsp;五月的Rng时隔三年再次在msi夺冠了。夺冠的晚上我参加了杭州后援粉丝会组织的线下观影，感觉非常好。左边是个一区的钻石老哥，右边是个铂金，我一个云玩家居然能和他们聊战局和BP聊得风生水起。看来看了这么多把比赛，理论经验还是足够丰富啊。后来钻石老哥看我人还行，随口问了句我段位，说可以回去一起玩。自己只好找个理由搪塞过去了，转移了话题。</p>
<img src="/2021/06/07/%E4%BA%94%E6%9C%88%E5%A4%A9/rng" style="zoom: 67%;">



<p>&emsp;&emsp;总之，五月就这么结束了，迎来了学期的最后一个月。希望这个六月能过得很充实。</p>]]></content>
      <categories>
        <category>Life</category>
        <category>Summary</category>
      </categories>
      <tags>
        <tag>Life chips</tag>
        <tag>Summary</tag>
      </tags>
  </entry>
  <entry>
    <title>高考小记</title>
    <url>/2021/06/05/%E9%AB%98%E8%80%83%E5%B0%8F%E8%AE%B0/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>&emsp;&emsp;高考前的一个月，或者说整个高三下，我的印象都非常清晰。</p>
<p>&emsp;&emsp;高三下我过得非常不好。就好像大家都在泳池的赛道里齐头并进的划水、蹬腿，自己却突然猛呛了一口水，开始挣扎，慢慢沉落。然而人们都忙着冲向终点，没有发觉这不起眼的小插曲、小浪花。</p>
<p>&emsp;&emsp;不过回想起自己的朋友，当初像我一样沉在水底的人也不少吧。</p>
<br>

<p>&emsp;&emsp;高考的前两月，每天清晨走在去吃早饭的小路上，都感觉前面有一行大大的“离高考还有xxx天”浮在空中，然后我“刷”的穿过这行字，带着奇异的心情开始一天的旅程。听课，做题，复习，总结… 高考前的每一天都很“满”，能做完作业已经是我的极限了，没有空去买额外的题集做练习。</p>
<p>&emsp;&emsp;那时杨老头长期给我们灌输心灵鸡汤：“只要努力就一定——”</p>
<p>&emsp;&emsp;本以为高三的最后会在平稳的复习中度过，但生活总是充满了许许多多的意外。成绩已经是起伏不定，然而其他方面雪上加霜。基本每天下午吃完晚饭，都会在到操场慢慢散步，或者坐电梯到行政楼六楼静静坐着。</p>
<p>&emsp;&emsp;成都那几个月的天气总是阴郁的。从六楼的窗户往外看，一只灰灰的鸽子抓在房檐上，眼珠凝视远方。于是顺着视线望去，便看到阴郁的天穹，笼罩这整个学校。天穹之下，穿着蓝白校服的小点在校园的柏油路上，各怀心事地匆匆走着，也不清楚未来将会奔向哪里。</p>
<p>&emsp;&emsp;那段日子让我渐渐开始讨厌成都这连绵不绝的阴郁，让人怎么也望不穿，反而让活在这片天空下的人发酵着情绪。那个时候中午经常放五月天的《一半人生》，有一句歌词我印象非常深刻：</p>
<p>&emsp;&emsp;我的一半人生 / 游荡就像是风筝</p>
<p>&emsp;&emsp;如果命运是风 / 什么又是我的绳</p>
<p>&emsp;&emsp;所以我们会被吹散到哪去呢？</p>
<br>

<p>&emsp;&emsp;高考的前一晚上，教室里的人各自怀揣着不同的心理，或安静复习，或和往常一样的扎堆聊天。我自己一个人在教室旁的杂物间，靠着栏杆里吃香蕉，试着放空。老实说当时的我并不紧张，但是有压力，毕竟可能是人生最重要的一场考试。</p>
<p>&emsp;&emsp;后来陈彦廷也进了杂物间。于是自然而然地开始讨论起明天的考试，讨论起高考，甚至想想将来。</p>
<p>&emsp;&emsp;他和我讲，这都是命。考的好考不好，不过是命里注定。</p>
<p>&emsp;&emsp;然而我吃着香蕉，什么也不清楚。但也不是没有自己的想法，而是不想去想那么多。还是吃完香蕉赶紧去复习吧，毕竟也做不了别的什么了。</p>
<p>&emsp;&emsp;晚上睡觉的时候其实倒也没有睡不着，但是没睡熟。中途醒了两次，也不清楚是为什么。</p>
<br>

<p>&emsp;&emsp;高考那天，老师们穿着清一色的红送我们进了考场。早上考完语文，整体也还算平滑。考试开始的时候感觉一点也不紧张，最后1分钟检查卷子的时候倒是心跳越来越快，怕机读卡或者身份信息哪里填错了，反复的检查，直到考试铃一响，虚出一身冷汗。</p>
<p>&emsp;&emsp;坐在我前面的小个子女生，长得特别像我的生物老师Lily。所以我暗暗称她为小Lily。旁边的考生考前一般在发呆或者玩手指，考完就像解放了一样迅速离开。只有小Lily考前正襟危坐，考后慢慢地收拾文具准备离开，让我印象很深刻。考完语文，我四下观察了小Lily以及考场里其他同学的反应，大多都还算放松。看来大家都觉得不是很难。我自己该答的都答了，作文写的很平庸，只能听天由命了。</p>
<p>&emsp;&emsp;下午的数学考的很简单，反而让我起了很重的疑心，前面的选择题做得很慢，导致最后几何的最后一题没有做完，实乃大失误。不过好在估摸了一下，也不会和别人拉开大差距，想想也就过去了。</p>
<p>&emsp;&emsp;晚上也是在正常的复习中度过。中途大家跑出教室到操场去散步（还是去摘桃？），我和少数几个同学留下来继续复习。当时觉得自己没什么好散心的，不如安心呆在教室里自习。</p>
<p>&emsp;&emsp;第二天的理综考的极差，化学有点小蒙，到了最后一秒还在纠结一个生物选择题，改了选项。下午的英语倒是四平八稳，觉得没什么大问题。整体高考下来，觉得卷子还是不算难的，可能就是比谁失误的少了吧。</p>
<p>&emsp;&emsp;没想到考完高考居然也没什么感觉，也没想去哪玩儿，也没回忆过去，和往常一样的离开考场。转念一想，我啥大风大浪没见过，高考不能引起我的心理波动。</p>
<p>&emsp;&emsp;走在去校车的路上，准备回学校，突然在另外一辆别的学校的校车上瞥见了小Lily。那辆校车上的人们看起来都在说着这两天的考试，或者过会儿准备去哪玩，热闹地扎堆讨论着，时不时传来男同学洪亮地声音。小Lily静静地坐在自己的座位上，头靠着窗户，看着外面的柏油路。</p>
<p>&emsp;&emsp;我看见她叹了一口气。</p>
<br>

<p>&emsp;&emsp;晚上回到家，临睡前刷了会儿B站。突然看到化学老师的QQ群里有人晒了化学的答案，一下子就清醒了。马上把答案对了一下，感觉很慌，错的不少。后面又给了各科答案的链接，于是心想，化学都对了，干脆就对完算了。于是一口气把答案对了遍。整体来说还算不错，不过那道最后临时改的生物选择果然从正确选项改成了错误选项，深为痛心。</p>
<p>&emsp;&emsp;对完了就睡不着了，开始估分，680？感觉不可能那么高。不过反正就是翻来覆去的睡不着，要么耍手机，要么又开始重新估分。于是乎就到了凌晨四点。最后把欠别人的一张生日贺卡补写了，终于倒在床上，慢慢等待睡梦的来临。</p>
<p>&emsp;&emsp;之后英语口语，散伙饭，毕业典礼，晚上和好友去吃饭唱K玩毕业真心话，其实每件事都可以拣来讲讲。很多人在高考之后，都彻底甩开了学习，做一些专属于青春的仪式，要么打算用自己的方式和高中告别，要么打算做一些自己一直想要做的事。而我要么听说，要么目睹了这种种的“仪式”，或者说是高考后终于冒出尖尖的心思。我也有我自己的想法，但我在很久之后才算真正完成了。不过既然完成了，也不算后悔。</p>
<br>

<p>&emsp;&emsp;出分的那天自己处于半紧张的状态。还没到查分的时候，就开始陆陆续续有各种信息像雨后春笋一样冒出来了，搞得我很烦。另外一边的陈昭是真紧张着急，我记得她那天下午和我聊天就一直处于一种焦虑的状态，担心成绩也担心未来。</p>
<p>&emsp;&emsp;很多人的分都出来了，都挺高，吓了我一跳，然而我老是登不上查分网，网页总是崩溃。</p>
<p>&emsp;&emsp;好不容易登上去了，于是我马上让父母别查，也让他们别找别人帮查，我自己一定一定要第一眼看我自己的成绩。于是跑到黑暗的卧室里开始输入身份信息。成绩页面出来的那一刻我瞬间用手盖住，那个时候心跳动的很厉害。于是慢慢把手往下移，看到语文125后开始有点小欣喜，然后就忍耐不住直接把手移开，看到了自己总分:691。</p>
<p>&emsp;&emsp;我突然想起来《你好旧时光》里写的余周周的查分的场景，于是像她一样，装作一脸沮丧的走出卧室回到客厅，妈妈看到我的表情感觉不对，担心的说了句“没考好啊？”。于是我把手机给她看，自己头低下去，开始暗暗窃喜。</p>
<p>&emsp;&emsp;“这个是多少… 天哪！”</p>
<br>

<p>&emsp;&emsp;那一晚上就是在亲人之间奔走相告，我只和我以前的关系很好的老师打了电话，报了喜讯。那天看分数的时候还觉得清北有望，复交保底。第二天早上看分段榜，四川有100多个700分以上的——原来是我想多了。</p>
<p>&emsp;&emsp;后来提前批没报上上交工科班，也不想去读自然和核物理，于是最后去了第一志愿的浙大工科。当时得知自己被录取的时候，我还在和同学在日本玩。知道了录取之后，抛下了去秋叶原shopping的同学，一个人安静的坐在东京的Palette Town，看着窗外的波光粼粼。</p>
<p>&emsp;&emsp;一切也算尘埃落定，过往云烟成雨。</p>
<br>

<p>&emsp;&emsp;高考对我来说更像是一段连续的时期，囊括了整个高三，一直到敲定学校的那一天。每一天好似如履薄冰，站在薄薄的冰面上摸索着前进的道路。每一步都要走的很谨慎，好像稍不注意就会跌破，然后整个冰面坍塌，人也就沉落了。所以每一步都要很小心，很谨慎，即使现在身体冰凉，抑或心态处于崩溃的边缘，也要努力控制自己，撑到跨越冰面的彼岸。</p>
<p>&emsp;&emsp;在日本毕业游玩的时候，晚上躺在床上往往不是想着白天的旅行，而是高中。那个时候的我觉得自己就像是风筝，不知道是什么绳子拴住了自己难以飞翔，也不知道命运的风会把我吹向哪里。但我选择了麻痹自己，一直投入在学习里面，每次考试后认真总结，尽力做好每次的作业，下课缠着老师问题…… 试着把所有的心思放在当下，不去思考周围，也不去思考未来。现在慢慢明白，人活着就像在一张图的某个结点上，有着无数个分支，无数条路，从当下前往不同的结点。人有概率的选择其中某一个分支，走向未来。即使不能百分百地选择我们最想走的那条路，但我当下要做的，就是把最想走的那条路的概率，让他无限趋近于1。我们为了高考不断地刷题，考试，的确也无法100%的考好，但我们可以追求那99.9999999%. 即使最后那0.0000001%发生了，对我来说，我也不后悔。</p>
<p>&emsp;&emsp;所以我想，也许我不是那歌词里的风筝。我们在命运的困境里总会把自己代入到别的悲情角色里，寻找着些许的认同和安慰。但后来发现，真正能让我自己重新振翅翱翔的，正是，也只能是，我自己。</p>
<br>

<p>&emsp;&emsp;几家欢喜几家愁是最适合拿来形容高考的。朋友中有厉害的却考得不好，有平常一般的却超常发挥的。杨老头当初说“只要努力就一定——”，像是失灵了。</p>
<p>&emsp;&emsp;但我想，也不算。也许当初我们对失败的定义就是高考考砸，就是与自己心仪的大学失之交臂。然而时至今日，却发现生活也还是照样的过。过去的成王败寇太过绝对，一路走来却也觉安知非福。我们19年高考的这一代在那个暑假，慢慢和过去的自己，过去的中学时光道别，踏上了崭新的征程，遇见新的故事和新的难题。现在回首当时如此重要的高考，其实也就是人生当中的一个分水岭，但并不是唯一的。厉害的同学到了普通的大学仍然散发着光芒，而那些去了Top院校的同学却也开始挣扎在保研和内卷的压力之中。身边的东西都在慢慢改变，但是没有改变的是，我们仍然地重复着，做99.99999999%的趋近，走自己最想要的那条路。无论过往多少次0.000000001%的发生，我们都不依不挠地努力着。</p>
<p>&emsp;&emsp;感谢过往当时那个缺课一年也不放弃自己的自己，感谢那个在高三下“溺水”却也拼命挣扎着要冲上对岸的自己。站在人生的这张充满概率的图上，我们不占有“只要努力就一定”的天赋，却拥有“只有努力才方可”的能力，不断趋近着，那个理想的”1“。</p>]]></content>
      <categories>
        <category>Recall</category>
        <category>Senior</category>
      </categories>
      <tags>
        <tag>Recall</tag>
        <tag>Gaokao</tag>
        <tag>Nostalgia</tag>
      </tags>
  </entry>
  <entry>
    <title>CS224W Sec 4~5</title>
    <url>/2021/05/31/CS224W-Sec-4-5/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h1 id="CS224W-Sec-4-5"><a href="#CS224W-Sec-4-5" class="headerlink" title="CS224W Sec 4~5"></a>CS224W Sec 4~5</h1><h2 id="Section-4"><a href="#Section-4" class="headerlink" title="Section 4"></a>Section 4</h2><h3 id="4-1-Pipeline"><a href="#4-1-Pipeline" class="headerlink" title="4.1 Pipeline"></a>4.1 Pipeline</h3><h4 id="PageRank"><a href="#PageRank" class="headerlink" title="PageRank"></a>PageRank</h4><ul>
<li><p>Idea: Use Link Analysis approaches to compute the importance of nodes</p>
<ul>
<li>Links as votes </li>
</ul>
</li>
<li><p>PageRank</p>
<ul>
<li><p>Flow Model</p>
<p>Each link’s vote is proportion to the importance of its source page.</p>
<p><img src="/2021/05/31/CS224W-Sec-4-5/flow.png"></p>
<ul>
<li>A page is important if it is pointed to by other important pages</li>
</ul>
</li>
<li><p>FlowModel: Use matrix to solve the equation instead of Gauss</p>
</li>
</ul>
</li>
<li><p>Matrix Formulation</p>
<ul>
<li><p>Stochastic adjacency matrix and rank vector</p>
<p><img src="/2021/05/31/CS224W-Sec-4-5/matrix.png"></p>
</li>
<li><p>Connection to Random Walk</p>
<p><img src="/2021/05/31/CS224W-Sec-4-5/p.png"></p>
<p>Then <code>p(t+1) = M * p(t)</code></p>
</li>
<li><p>Stationary Distribution: <code>p(t) = M * p(t)</code></p>
</li>
<li><p>If r is the limit of M M M M M u, then r satisfies the flow equation 1 * r = M * r; So r is the principal eigenvector.</p>
<ul>
<li>Solution: Power iteration</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="4-1-Summary"><a href="#4-1-Summary" class="headerlink" title="4.1 Summary"></a>4.1 Summary</h3><ul>
<li>Measures importance of nodes in a graph through link structure</li>
<li>Models a random web surfer using the <strong>stochastic adjacency matrix M</strong></li>
<li>PageRank solves r = Mr where r can views as both the stationary distribution of a random work and the principle eigenvector of M.                       </li>
</ul>
<p>​    </p>
<h3 id="4-2-Pipeline"><a href="#4-2-Pipeline" class="headerlink" title="4.2 Pipeline"></a>4.2 Pipeline</h3><h4 id="PageRank-How-to-solve"><a href="#PageRank-How-to-solve" class="headerlink" title="PageRank: How to solve"></a>PageRank: How to solve</h4><ul>
<li><p>How to solve the PageRank equation</p>
<ul>
<li><p>Assign each node an initial page rank</p>
</li>
<li><p>Repeat <code>p(t+1)=p(t) * M</code> until convergence</p>
<p><img src="/2021/05/31/CS224W-Sec-4-5/power.png"></p>
</li>
</ul>
</li>
<li><p>Question</p>
<ul>
<li>Does this converge</li>
<li>Does it converge to what we want?</li>
<li>Are results reasonable?</li>
</ul>
</li>
<li><p>Two problems in converge</p>
<ul>
<li>Spider Trap<ul>
<li>Importance flows to one node</li>
</ul>
</li>
<li>Dead end<ul>
<li>Some nodes has no out links, and the importance continuously reduces</li>
</ul>
</li>
</ul>
</li>
<li><p>Solutions:</p>
<ul>
<li><p>Spider Trap: At each time step, the random surfer has two options</p>
<ul>
<li>With prob. β, follow a link at random</li>
<li>with prob. 1 - β, jump to a random page</li>
</ul>
</li>
<li><p>Dead ends: Follow random teleport links with total prob. 1.0 from dead-ends</p>
</li>
<li><p>Fixed PageRank Equation:</p>
<p><img src="/2021/05/31/CS224W-Sec-4-5/equation.png"></p>
</li>
<li><p>Example:</p>
<p><img src="/2021/05/31/CS224W-Sec-4-5/example.png"></p>
</li>
</ul>
</li>
</ul>
<h3 id="4-2-Summary"><a href="#4-2-Summary" class="headerlink" title="4.2 Summary"></a>4.2 Summary</h3><ul>
<li>PageRank solves for r = Gr and can be efficiently computed by power iteration</li>
<li>Adding random uniform teleportation</li>
</ul>
<h3 id="4-3-Pipeline"><a href="#4-3-Pipeline" class="headerlink" title="4.3 Pipeline"></a>4.3 Pipeline</h3><h4 id="Random-Walk-with-Restarts"><a href="#Random-Walk-with-Restarts" class="headerlink" title="Random Walk with Restarts"></a>Random Walk with Restarts</h4><ul>
<li><p>Goal: Proximity (Related Recommendation)</p>
<ul>
<li>How to define a metric?(Shortest path, neighbors…)</li>
</ul>
</li>
<li><p>Personalized PageRank:</p>
<ul>
<li>Ranks proximity of nodes to the teleport nodes S</li>
<li>Q: What is most related item to <strong>Item Q</strong></li>
<li>Random Walks with Restarts:<ul>
<li>Teleport back to the starting node: S ={Q}</li>
<li>Tips: S = V previously(In 4.2 random walks)</li>
</ul>
</li>
</ul>
</li>
<li><p>Pixie Random Walk Algorithm(On bipartite)</p>
<p><img src="/2021/05/31/CS224W-Sec-4-5/Palgorithm.png"></p>
<ul>
<li><p>Higher visit counts means:</p>
<ul>
<li>More paths</li>
<li>More common neighbors</li>
</ul>
</li>
<li><p>You can <strong>also use power iteration</strong> to achieve</p>
</li>
</ul>
<h3 id="4-3-Summary"><a href="#4-3-Summary" class="headerlink" title="4.3 Summary"></a>4.3 Summary</h3></li>
<li><p>PageRank:</p>
<ul>
<li>Teleports to any node</li>
<li>Nodes can have the same probability of the surfer landing</li>
</ul>
</li>
<li><p>Topic Specific PageRank aka Personalized PageRank</p>
<ul>
<li>Teleports to a specific set of nodes</li>
<li>Have different probabilities</li>
</ul>
</li>
<li><p>Random Walk with Restarts</p>
<ul>
<li>Teleports to the same node</li>
</ul>
</li>
</ul>
<h3 id="4-4-Pipeline"><a href="#4-4-Pipeline" class="headerlink" title="4.4 Pipeline"></a>4.4 Pipeline</h3><h4 id="Embeddings-amp-Matrix-Factorization"><a href="#Embeddings-amp-Matrix-Factorization" class="headerlink" title="Embeddings &amp; Matrix Factorization"></a>Embeddings &amp; Matrix Factorization</h4><ul>
<li><p>Connection to Matrix Factorization(<strong>Important： Z^T^Z=A</strong>)</p>
<p><img src="/2021/05/31/CS224W-Sec-4-5/connect.png"></p>
</li>
<li><p>Not Impossible: The embedding dimension d &lt;&lt; n</p>
<ul>
<li>But we can learn <strong>Z</strong> approximately<ul>
<li>min ||A - Z^T^Z||<del>2</del></li>
<li>Based on connectivity</li>
</ul>
</li>
</ul>
</li>
<li><p>DeepWalk and node2vec have a more complex node similarity definition based on <strong>random walks</strong></p>
</li>
</ul>
<ul>
<li>Limitation of node embeddings via matrix factorization and random walks<ul>
<li>Can’t obtain embeddings for nodes not in the training set<ul>
<li>Need to recompute all node embeddings once added</li>
</ul>
</li>
<li>Can’t capture structural similarities<ul>
<li>If two node u, v have similarities structurally but there is a long distance between them, it’s unlikely that a random walk will reach v from u.</li>
</ul>
</li>
<li>Can’t utilize node, edge and graph features</li>
</ul>
</li>
<li>Solution: Deep Representation Learning and GNN</li>
</ul>
<p><strong>Viewing graphs as matrices play a key role in all above algorithms</strong></p>
<h2 id="Section-5"><a href="#Section-5" class="headerlink" title="Section 5"></a>Section 5</h2><h3 id="5-1"><a href="#5-1" class="headerlink" title="5.1"></a>5.1</h3><h4 id="Message-Passing-and-Node-Classification"><a href="#Message-Passing-and-Node-Classification" class="headerlink" title="Message Passing and Node Classification"></a>Message Passing and Node Classification</h4><ul>
<li><p>Question: Given a network with labels on some nodes, how do we assign labels to all other nodes in the network?</p>
<ul>
<li>Solution: Node Embeddings method</li>
</ul>
</li>
<li><p>Different situation: Semi-supervised node classification</p>
<ul>
<li>Semi-supervised: Some labels are given while other are not.</li>
</ul>
</li>
</ul>
<p>An alternative framework: message passing</p>
<ul>
<li><p>Intuition: <strong>Correlations</strong> exist in networks</p>
<ul>
<li>Similar nodes are connected</li>
<li>Collective Classification</li>
</ul>
</li>
<li><p>Three Classical Techniques:</p>
<ul>
<li><strong>Relational classification</strong></li>
<li><strong>Iterative classification</strong></li>
<li><strong>Belief propagation</strong></li>
</ul>
</li>
<li><p>Some notions</p>
<ul>
<li>Homophily: The tendency of individuals to associate and bond with similar others<ul>
<li>Examples: Social Network(With the same interest, people are more closely connected)</li>
</ul>
</li>
<li>Influence: Social connections can influence the individual characteristics of a person<ul>
<li>Examples: Recommend movie, music….</li>
</ul>
</li>
</ul>
</li>
<li><p>Motivation:</p>
<ul>
<li>Guilt-by-association: If I am connected to a node with label X, then I am likely to have label X as well</li>
<li>Classification label of a node v may depend on<ul>
<li>Features of v</li>
<li>Labels of the nodes in v’s neighborhood</li>
<li>Features of the nodes in v’s neighborhood </li>
</ul>
</li>
</ul>
</li>
<li><p>Collective Classification:</p>
<ul>
<li><p>Intuition: Using correlations</p>
</li>
<li><p>Markov Assumption: The label Y<del>v</del> of one node v depends on the labels of it’s neighbors.</p>
<p>P(Y<del>v</del>) = P(Y<del>v</del> | N<del>v</del>)</p>
</li>
<li><p>Steps:</p>
<ul>
<li><p>Local Classifier: Used for initial label assignment</p>
</li>
<li><p>Relational Classifier: Capture correlations(Learn)</p>
</li>
<li><p>Collective Inference : Propagate the correlation</p>
<p><strong>Apply relational classifier to each node iteratively until converge</strong></p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="5-2"><a href="#5-2" class="headerlink" title="5.2"></a>5.2</h3><h4 id="Relational-and-Iterative-classification"><a href="#Relational-and-Iterative-classification" class="headerlink" title="Relational and Iterative classification"></a>Relational and Iterative classification</h4><ul>
<li><p>Probabilistic <strong>Relational Classifier</strong>(Initialize the unlabeled nodes with 0.5(prob. =class 1))</p>
<p><img src="/2021/05/31/CS224W-Sec-4-5/relational.png"></p>
<p>Example: P24</p>
</li>
</ul>
<p>How to use <strong>node attributes</strong> in our algorithm?</p>
<ul>
<li><p><strong>Iterative Classification</strong></p>
<p><img src="/2021/05/31/CS224W-Sec-4-5/iterative.png"></p>
<p>z<del>v</del>（summary vector）: Histogram of the number (or fraction) of each label in N<del>v</del></p>
</li>
<li><p>Architecture</p>
<p><img src="/2021/05/31/CS224W-Sec-4-5/architecture.png"></p>
<p>How to train classifier has not introduced yet.</p>
<p>Example: P39</p>
</li>
<li><p>Steps:</p>
<ul>
<li>Train classifier</li>
<li>Apply classifier to test</li>
<li>Iterate<ul>
<li>Update z<del>v</del></li>
<li>Update label Y<del>v</del></li>
</ul>
</li>
</ul>
<p>M’s Intuition: Classifier may have a initial mechanism to predict Y<del>v</del>, so the z<del>v</del> can be filled initially and the training can start.</p>
</li>
</ul>
<p>M’s Q: If it reaches the maximum iteration times but still can’t converge, will it stabilize?(in a small fluctuation)</p>
<h3 id="5-3"><a href="#5-3" class="headerlink" title="5.3"></a>5.3</h3><h4 id="Loopy-Belief-Propagation"><a href="#Loopy-Belief-Propagation" class="headerlink" title="Loopy Belief Propagation"></a>Loopy Belief Propagation</h4><p>Loopy? : Apply to the graph has cycles</p>
<ul>
<li><p>Message Passing: Basics</p>
<p><img src="/2021/05/31/CS224W-Sec-4-5/basic.png"></p>
<p>Extension: On Tree(Like the flash back of DFS)</p>
</li>
<li><p>Notations:</p>
<p><img src="/2021/05/31/CS224W-Sec-4-5/notation.png"></p>
</li>
<li><p>Loopy BP algorithm</p>
<p><img src="/2021/05/31/CS224W-Sec-4-5/loopy.png" alt="image-20210606210618137"></p>
<p>We can see that, through label-label potential, we <strong>collect all the possibilities that might occur</strong>, not like the previous correlation(homophily).</p>
</li>
<li><p> Have Cycles?</p>
</li>
<li><p>What can go wrong: Beliefs may not converge</p>
</li>
<li><p>In practice: Loopy BP is still good for complex graphs which contain many branches</p>
</li>
<li><p>In some extreme situations: Message loops</p>
</li>
<li><p>Summary</p>
<p><img src="/2021/05/31/CS224W-Sec-4-5/sum.png"></p>
</li>
</ul>
<h2 id="Words"><a href="#Words" class="headerlink" title="Words"></a>Words</h2><p>navigational: 导航的</p>
<p>citation: 引用</p>
<p>encyclopedia:[ɪnˌsaɪkləˈpiːdiə]: 百科全书</p>
<p>trustworthy: 值得信任的，可靠的</p>
<p>stochastic: 随机的</p>
<p>surfer: 漫游者</p>
<p>hypothetical: 假定的</p>
<p>stabilize: 稳定</p>
<p>eigenvector: 特征向量</p>
<p>converge: 集中，相似，相同</p>
<p>teleport: 传送</p>
<p>leak out: 露出，走风，泄露</p>
<p>metric: 标准</p>
<p>evenly: 平滑的，有规律的，均匀的</p>
<p>limiting distribution: 极限分布(随机变量列的极限的概率分布)</p>
<p>matrix factorization: 矩阵分解</p>
<p>discrepancy: 差异</p>
<p>albeit:[ˌɔːlˈbiːɪt]:虽然，尽管</p>
<p>intermediate: 中间的</p>
<p>correlation: 相关性</p>
<p>leverage: 利用</p>
<p>probabilistic: 基于概率的</p>
<p>ground-truth: 基本事实</p>
<p>likelihood: 可能性</p>
<p>histogram: 直方图</p>
<p>notation: 符号</p>
<p>heuristic: [hjuˈrɪstɪk]: adj. 启发式的</p>]]></content>
      <categories>
        <category>Research</category>
        <category>Learning</category>
        <category>Notes</category>
      </categories>
      <tags>
        <tag>Graph</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>CS224W Lab1</title>
    <url>/2021/05/30/CS224W-Lab1/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h1 id="CS-224W-Lab1"><a href="#CS-224W-Lab1" class="headerlink" title="CS 224W Lab1"></a>CS 224W Lab1</h1><h2 id="Question-1"><a href="#Question-1" class="headerlink" title="Question 1"></a>Question 1</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">avg_degree = <span class="built_in">round</span>(num_edges * <span class="number">2</span> / num_nodes)</span><br></pre></td></tr></table></figure>
<p><strong>Note</strong>: Use Function <code>round()</code> to round the result.</p>
<h2 id="Question-2"><a href="#Question-2" class="headerlink" title="Question 2"></a>Question 2</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">avg_cluster_coef = <span class="built_in">round</span>(nx.average_clustering(G), <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>About clustering coefficient:</p>
<p><img src="/2021/05/30/CS224W-Lab1/image-20210528172335527.png" alt="image-20210528172335527"></p>
<p>Call the function of NetworkX to calculate the answer.</p>
<h2 id="Question-3"><a href="#Question-3" class="headerlink" title="Question 3"></a>Question 3</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> ni <span class="keyword">in</span> nx.neighbors(G, node_id):</span><br><span class="line">  r1 += beta * r0 / G.degree[ni]</span><br><span class="line">r1 += (<span class="number">1</span> - beta) * r0</span><br><span class="line">r1 = <span class="built_in">round</span>(r1, <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>Pay attention to that: we only need to calculate <code>PageRank</code> of <code>node 0</code> after one iteration. So we could use r<del>0</del> to replace r<del>i</del> .</p>
<p>More specific algorithm:</p>
<p><img src="/2021/05/30/CS224W-Lab1/image-20210528210137487.png" alt="image-20210528210137487"></p>
<h2 id="Question-4"><a href="#Question-4" class="headerlink" title="Question 4"></a>Question 4</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sigma_shortd = nx.shortest_path_length(G, source = node)</span><br><span class="line"><span class="built_in">sum</span> = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> ni <span class="keyword">in</span> <span class="built_in">range</span> (G.number_of_nodes()):</span><br><span class="line">  <span class="keyword">if</span> (ni != node):</span><br><span class="line">    <span class="built_in">sum</span> += sigma_shortd[ni]</span><br><span class="line">closeness = <span class="built_in">round</span>(<span class="number">1</span> / <span class="built_in">sum</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>or</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">closeness = nx.closeness_centrality(G, u = node) / (G.number_of_nodes() - <span class="number">1</span>)</span><br><span class="line"><span class="comment">#Divide n-1 (n=nodes that u could reach). See the docs</span></span><br></pre></td></tr></table></figure>
<p>The docs for function: <a href="https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.shortest_paths.generic.shortest_path_length.html">shortest_path_length</a></p>
<p>The docs for function: <a href="https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.centrality.closeness_centrality.html?highlight=closenes#networkx.algorithms.centrality.closeness_centrality">closeness_centrality</a></p>
<h2 id="Question-5"><a href="#Question-5" class="headerlink" title="Question 5"></a>Question 5</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">edge_list = <span class="built_in">list</span>(G.edges())</span><br></pre></td></tr></table></figure>
<p>The return type of G.edges() is EdgeView. Usually iterates as tuple(u, v) or (u, v, d). (default = false , 2 tuple). Can also be used for attribute look up as <code>edges[u, v][&#39;foo&#39;]</code>.</p>
<p>Using append to add element is also feasible.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">edge_index=torch.LongTensor(edge_list).t()</span><br></pre></td></tr></table></figure>
<p>t() is to transpose.</p>
<h2 id="Question-6"><a href="#Question-6" class="headerlink" title="Question 6"></a>Question 6</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">neg_edge_list = <span class="built_in">list</span>(nx.non_edges(G))</span><br><span class="line">neg_edge_list = random.sample(neg_edge_list, num_neg_samples)</span><br></pre></td></tr></table></figure>
<p>Use the function in NetworkX to get the edges that not exist and random.sample to sample the neg_edge_list.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">check</span>(<span class="params">cedge</span>):</span>    <span class="keyword">return</span> cedge <span class="keyword">in</span> neg_edge_list <span class="keyword">or</span> (cedge[<span class="number">1</span>], cedge[<span class="number">0</span>]) <span class="keyword">in</span> neg_edge_listprint(check(edge_1))print(check(edge_2))print(check(edge_3))print(check(edge_4))print(check(edge_5))</span><br></pre></td></tr></table></figure>
<p>It’s undirected, so (u, v) and (v, u) is identical.</p>
<h2 id="Question-7"><a href="#Question-7" class="headerlink" title="Question 7"></a>Question 7</h2><p>Note: <code>Sklearn</code> is a library for many aspects of machine learning.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">emb = nn.Embedding(num_node, embedding_dim)  emb.weight.data = torch.rand(num_node, embedding_dim)</span><br></pre></td></tr></table></figure>
<p><code>torch.rand()</code> include a group of random numbers under [0, 1) uniform distribution.</p>
<p><code>pca = PCA(n_components= x)</code> The dimension will be reduced to x.</p>
<p>Use <code>plt.scatter</code> to draw scatter diagram.</p>
<p>numpy()  is to transform the data type.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">accu = <span class="built_in">round</span>(((pred &gt; <span class="number">0.5</span>) == label).<span class="built_in">sum</span>().item() / (pred.shape[<span class="number">0</span>]), <span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<p>I learned this way from github(By <strong>Polaris</strong>): pred == label will not return a boolean value but a tensor. After sum(), it will become a tensor with only one value. Then use item() to extract this value as a scalar.</p>
<ul>
<li>epoch: All the data in the sample are computed for once is called a epoch</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer.zero_grad()  </span><br><span class="line">train_node_emb=emb(train_edge)</span><br><span class="line">dot_product_result=train_node_emb[<span class="number">0</span>].mul(train_node_emb[<span class="number">1</span>])</span><br><span class="line">dot_product_result=torch.<span class="built_in">sum</span>(dot_product_result,<span class="number">1</span>) </span><br><span class="line">sigmoid_result=sigmoid(dot_product_result) </span><br><span class="line">loss_result=loss_fn(sigmoid_result,train_label)  </span><br><span class="line">loss_result.backward()</span><br><span class="line">optimizer.step()  </span><br><span class="line">print(loss_result) </span><br><span class="line">print(accuracy(sigmoid_result,train_label))</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Research</category>
        <category>Learning</category>
        <category>Projects</category>
      </categories>
      <tags>
        <tag>Graph</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>CS224W Lab0</title>
    <url>/2021/05/30/CS224W-Lab0/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h1 id="CS224-W-Lab0"><a href="#CS224-W-Lab0" class="headerlink" title="CS224 W Lab0"></a>CS224 W Lab0</h1><ul>
<li><p>Introduce some basic concepts of graph mining and Graph Neural Networks.</p>
</li>
<li><p>Two Packages:</p>
<ul>
<li><p>NetworkX</p>
<p>For Graph: <a href="https://networkx.org/documentation/stable/reference/classes/index.html">NetworkX graph types</a></p>
<p><img src="/2021/05/30/CS224W-Lab0/image-20210527212511812.png" alt="image-20210527212511812"></p>
</li>
<li><p>PyTorch Geometric</p>
</li>
</ul>
</li>
</ul>
<h2 id="NetworkX-Tutorial"><a href="#NetworkX-Tutorial" class="headerlink" title="NetworkX Tutorial"></a>NetworkX Tutorial</h2><p>NetworkX is one of the most frequently used Python packages to create, manipulate, and mine graphs.</p>
<ul>
<li><p><code>G = nx.Graph</code></p>
<p>Create a new empty undirected graph. It could also be:</p>
<p><code>G = nx.DiGraph</code> etc.</p>
</li>
<li><p><code>G.is_directed</code></p>
<p>To judge whether it’s directed graph, return the boolean value.</p>
</li>
<li><p><code>G.graph[&quot;name&quot;] = &quot;Bar&quot;</code></p>
<p>Add attribute pair.</p>
</li>
<li><p><code>G.add_node(node_for_adding, **attr)</code></p>
<p>Add the node and update node attributes(could follow more than 1).</p>
<ul>
<li>New way for <code>print</code>: <code>print.format</code> by <code>&#123;&#125;</code></li>
</ul>
</li>
<li><p><code>G.add_nodes_from()</code></p>
<p>A more flexible way to add nodes. (Add list or something)</p>
<p><img src="/2021/05/30/CS224W-Lab0/image-20210527214842221.png" alt="image-20210527214842221"></p>
<ul>
<li><p>About <code>(data=True)</code></p>
<p>It could return the attributes information of node when printing.<strong>(Need deeper understanding here)</strong></p>
<p><img src="/2021/05/30/CS224W-Lab0/image-20210527215110377.png" alt="image-20210527215110377"></p>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>About Edge</p>
<p>Change the <code>node_for_adding</code> to a pair of nodes.</p>
</li>
</ul>
<ul>
<li><p>Visualization</p>
<p><code>nx.draw(G, with_labels = True)</code></p>
</li>
</ul>
<ul>
<li>Other Functions<ul>
<li><code>G.degree[node_id]</code>: Get the degree of node_id</li>
<li><code>G.neighbors(node_id)</code>: Get the set of the neighbors of node_id</li>
<li>More Information: <a href="https://networkx.org/documentation/stable/">Click Here</a></li>
</ul>
</li>
</ul>
<h2 id="PyTorch-Geometric-Tutorial"><a href="#PyTorch-Geometric-Tutorial" class="headerlink" title="PyTorch Geometric Tutorial"></a>PyTorch Geometric Tutorial</h2><p>PyTorch Geometric (PyG) is an extension library for PyTorch. It provides useful primitives to develop Graph Deep Learning models, including various graph neural network layers and a large number of benchmark datasets.</p>
<p><strong>Wait for filling…</strong></p>]]></content>
      <categories>
        <category>Research</category>
        <category>Learning</category>
        <category>Projects</category>
      </categories>
      <tags>
        <tag>Graph</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>CS224W Sec.1~3</title>
    <url>/2021/05/30/CS224W-Sec-1-3/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h1 id="CS-224W-Sec1-3"><a href="#CS-224W-Sec1-3" class="headerlink" title="CS 224W Sec1~3"></a>CS 224W Sec1~3</h1><h2 id="Section-1"><a href="#Section-1" class="headerlink" title="Section 1"></a>Section 1</h2><h3 id="Pipeline"><a href="#Pipeline" class="headerlink" title="Pipeline"></a>Pipeline</h3><p>It gives a brief introduction to the course and some basic concepts of Graph.</p>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><ul>
<li>Machine learning with Graphs<ul>
<li>Applications and use cases</li>
</ul>
</li>
<li>Different types of tasks<ul>
<li>Node level</li>
<li>Edge level</li>
<li>Graph level</li>
</ul>
</li>
<li>Choice of a graph representation<ul>
<li>Directed, undirected, bipartite, weighted, self edges, multigraph</li>
<li>Adjacency matrix, edge list, adjacency list</li>
<li>Connectivity(Scc,etc.)</li>
</ul>
</li>
</ul>
<h2 id="Section-2"><a href="#Section-2" class="headerlink" title="Section 2"></a>Section 2</h2><p>Waiting for summary.</p>
<h2 id="Section-3"><a href="#Section-3" class="headerlink" title="Section 3"></a>Section 3</h2><h3 id="3-1-Pipeline"><a href="#3-1-Pipeline" class="headerlink" title="3.1 Pipeline"></a>3.1 Pipeline</h3><p>Use Representation Learning to replace Feature Engineering: <em><strong>Auto</strong></em> replace <em><strong>Manual</strong></em>.</p>
<p><img src="/2021/05/30/CS224W-Sec-1-3/image-20210529192748944.png" alt="image-20210529192748944"></p>
<p>The vector that is extracted from Graph is called Feature representation, or <em><strong>Embedding</strong></em>.</p>
<p><img src="/2021/05/30/CS224W-Sec-1-3/image-20210529193143639.png" alt="image-20210529193143639"></p>
<p><strong>Goal: Map nodes into an embedding space, to approximate</strong></p>
<p><img src="/2021/05/30/CS224W-Sec-1-3/image-20210529193411637.png" alt="image-20210529193411637"></p>
<p><strong>Steps:</strong></p>
<ol>
<li>Encoder maps from nodes to embeddings</li>
<li>Define a node similarity function</li>
<li>Decoder maps from embeddings to the similarity score</li>
<li>Optimize the parameters of the encoder so that: The similarity of the original network approximate the similarity of embedding**(Usually as dot(Z<del>v</del>^T^Z<del>u</del>))**</li>
</ol>
<p>Many method: <strong>DeepWalk</strong>, node2vec</p>
<p>Node similarity:</p>
<ul>
<li> Are linked?</li>
<li> Share neighbors?</li>
<li> Have similar “structural roles “?(like graphlet, kernel)</li>
</ul>
<p>Use <strong>random walks</strong> to learn node similarity definition.</p>
<p>These embeddings are <strong>task independent</strong>, without utilizing node labels and features</p>
<h3 id="3-1-Summary"><a href="#3-1-Summary" class="headerlink" title="3.1 Summary"></a>3.1 Summary</h3><p>Encoder + Decoder Framework</p>
<ul>
<li>Shallow encoder: embedding lookup</li>
<li>Parameters to optimize: Z which contains node embeddings z<del>u</del> for all nodes u ∈ V</li>
<li>Deep encoders(GNNs) will be covered in Lecture 6</li>
</ul>
<ul>
<li>Decoder : Based on node similarity</li>
<li>Objective: maximize z<del>v</del>^T^z<del>u</del> for node pairs (<em>u, v</em>) that are similar</li>
</ul>
<h3 id="3-2-Pipeline"><a href="#3-2-Pipeline" class="headerlink" title="3.2 Pipeline"></a>3.2 Pipeline</h3><ul>
<li><p>Notation - Tired to type</p>
<p><img src="/2021/05/30/CS224W-Sec-1-3/image-20210529203256631.png"></p>
</li>
<li><p>Random-Walk</p>
<p><img src="/2021/05/30/CS224W-Sec-1-3/image-20210529204527933.png"></p>
<ul>
<li><p>Definition: A walk at random</p>
</li>
<li><p>Random-Walk Embedding:</p>
<p>z<del>u</del>^T^z<del>v</del> ≈ probability that u and v co-occur on a random walk over the graph</p>
</li>
<li><p>Expressivity: Incorporates both local and higher-order neighborhood information</p>
<ul>
<li>Idea: If random walk starting from node u visits v with higher probability, u and v are similar(higher-order multi-hop information)</li>
</ul>
</li>
<li><p>Efficiency: Only need to consider pairs that co-occur on random walks</p>
</li>
</ul>
</li>
<li><p>Feature Learning as Optimization</p>
<p><img src="/2021/05/30/CS224W-Sec-1-3/image-20210529204955162.png"></p>
</li>
<li><p>Steps:</p>
<ol>
<li>Run <strong>short fixed-length random walks</strong> (By strategy R)</li>
<li>For each node u collect N<del>R</del>(u) as <u>multiset</u> (can have repeated elements since nodes can be visited multiple times on random walks)</li>
<li>Optimize the function above -&gt; Maximum likelihood objective</li>
</ol>
</li>
<li><p>Use softmax to parameterize P(v | z<del>u</del>)</p>
<ul>
<li><img src="/2021/05/30/CS224W-Sec-1-3/image-20210529211134940.png" alt="image-20210529211134940"></li>
</ul>
</li>
</ul>
<p><strong>Basic Function</strong>:</p>
<p><img src="/2021/05/30/CS224W-Sec-1-3/image-20210529211453112.png" alt="image-20210529211453112"></p>
<p>Improvement:</p>
<p><img src="/2021/05/30/CS224W-Sec-1-3/image-20210529212644608.png"></p>
<p>In practice, k = 5 ~ 20. </p>
<ul>
<li>Higher k gives more robust estimates</li>
<li>Higher k corresponds to higher bias on negative events</li>
</ul>
<p>Optimize the final function by: <strong>Stochastic Gradient Descent</strong></p>
<p>How to random walk?</p>
<ul>
<li><p>Key observation: Flexible notion of network neighborhood N<del>R</del>(u) of node u leads to rich node embeddings</p>
</li>
<li><p>node2vec: Biased Walks</p>
<ul>
<li>Idea: Use flexible, biased random walks that can trade off between <strong>local</strong> and <strong>global</strong> view of the network.</li>
<li>Two classic strategies: BFS and DFS</li>
</ul>
</li>
<li><p>Define two parameters: <em>p</em> and <em>q</em></p>
<p><img src="/2021/05/30/CS224W-Sec-1-3/image-20210529215517966.png" alt="image-20210529215517966"></p>
</li>
</ul>
<h3 id="3-2-Summary"><a href="#3-2-Summary" class="headerlink" title="3.2 Summary"></a>3.2 Summary</h3><ul>
<li>Core idea: Embed nodes so that distances in embedding space reflect node similarities in the originial network.</li>
<li>Notion of node similarity<ul>
<li>Naive: similar if 2 nodes are connected</li>
<li>Neighborhood overlap(Section 2)</li>
<li>Random walk approaches(Section 3)</li>
</ul>
</li>
<li>Method<ul>
<li>No one method wins in all cases<ul>
<li>node2vec performs better on node classification</li>
</ul>
</li>
<li>Must choose definition of node similarity that matches your application</li>
</ul>
</li>
</ul>
<h3 id="3-3-Pipeline"><a href="#3-3-Pipeline" class="headerlink" title="3.3 Pipeline"></a>3.3 Pipeline</h3><ul>
<li><p>Goal: Graph -&gt; Embedding</p>
<ul>
<li><p>Idea 1: Sum(or average) the node embeddings in G</p>
<ul>
<li>Used in 2016 to classify molecules based on graph structure(successful)</li>
</ul>
</li>
<li><p>Idea 2: Add virtual node to connect the graph and run embedding technique(like deep walk and so on), and use the embedding of this virtual node</p>
</li>
<li><p>Idea 3: Anonymous Walk Embedding(Record by index)</p>
<ul>
<li>Agnostic to the identity of nodes visited</li>
</ul>
<p><img src="/2021/05/30/CS224W-Sec-1-3/image-20210530143442456.png" alt="image-20210530143442456"></p>
<ul>
<li><p>Represent the graph as a probability distribution over these walks(Grows exponentially)</p>
<p><img src="/2021/05/30/CS224W-Sec-1-3/image-20210530143934648.png" alt="image-20210530143934648"></p>
</li>
<li><p>The number of walks we need: see slide P58</p>
</li>
<li><p>How to? : See slide(Quite complex to note)</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="3-3-Summary"><a href="#3-3-Summary" class="headerlink" title="3.3 Summary"></a>3.3 Summary</h3><ul>
<li>Approach 1: Aggregate node embeddings</li>
<li>Approach 2: Create super-node or virtual node</li>
<li>Approach 3: Anonymous Walk Embedding<ul>
<li>Sample the walks and represent the graph as fraction of probability</li>
<li>Embed anonymous walks</li>
</ul>
</li>
</ul>
<h2 id="End"><a href="#End" class="headerlink" title="End"></a>End</h2><p>Quick view of machine learning (From website)</p>
<p><img src="/2021/05/30/CS224W-Sec-1-3/image-20210530202849172.png" alt="image-20210530202849172"></p>
<h2 id="Words"><a href="#Words" class="headerlink" title="Words"></a>Words</h2><p>alleviate: [əˈliːvieɪt] : 缓解，减轻</p>
<p>shallow : 浅的</p>
<p>supervise: 监督</p>
<p>co-occur:  共现</p>
<p>stochastic: [stə’kæstɪk] : 随机的，有可能性的</p>
<p>incorporate: 将…包括在内，包含，吸收，使并入</p>
<p>expressivity: 表现度</p>
<p>intuition: 直觉 (a way of ML)</p>
<p>likelihood: 可能， 可能性</p>
<p>objective: 目标</p>
<p>culprit: [ˈkʌlprɪt] : 罪魁祸首</p>
<p>uniform: 一致的，统一的</p>
<p>biased: 偏向性的</p>
<p>proportional: 成比例的，相称的</p>
<p>convergence：趋同，融合</p>
<p>derivative: n.派生物 adj. 缺乏独创性的</p>
<p>tune: 调整</p>
<p>constrained: 不自然的，过于受约束的</p>
<p>micro(macro)-view: 微观（宏观）视图</p>
<p>mimic: 模仿</p>
<p>overlap: 重叠</p>
<p>agnostic: 不可知论的</p>
<p>cluster: 聚集</p>
<p>concatenate: 连锁</p>]]></content>
      <categories>
        <category>Research</category>
        <category>Learning</category>
        <category>Notes</category>
      </categories>
      <tags>
        <tag>Graph</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>四月小结</title>
    <url>/2021/05/17/%E5%9B%9B%E6%9C%88%E5%B0%8F%E7%BB%93/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=1373228032&auto=1&height=66"></iframe>

<h2 id="课程"><a href="#课程" class="headerlink" title="课程"></a>课程</h2><p>&emsp;&emsp;四月是春学期的最后一个月。</p>
<p>&emsp;&emsp;法学通论是短学期课程，期末作业要交两篇论文。第一篇是关于一个胎盘归属问题的答复，第二篇是关于许霆案的“极限正义”讨论问题。第一篇是个民法的问题，老师给了一些提示，不过对于我这种门外汉来说还是一窍不通。上课的确有很多有趣的案例分析，不过还是比较浅显，感觉不得要义，也怕写论文的时候显得像个法盲。于是把案例发给了现在在上海学法律的高中同桌zhy，和他qq讨论了一番，再打了一通电话，总算是把案件的论点和逻辑整理的比较清楚了。</p>
<p>&emsp;&emsp;第二篇的“极限正义”就是真的蒙圈了，从来没听过这个概念。第二篇是讲刑法的老师出的题，于是就去问zhy刑法里面有没有极限正义的概念。结果他说他也没学过。好在想起了刑法课刚开始的时候，讲刑法的老师提过他最近出的一本书就叫《极限正义》。赶紧在微店上找到了这本书并下单，收到了打开一翻：果然有关于许霆案的分析！也算是偷看了老师自己写的答案了。仔细读懂了极限正义的定义和老师书里的分析，按照自己的思路写了论文，把第二篇也交了上去。</p>
<p>&emsp;&emsp;最后出分只有92，本以为偷看了”答案“也许可以满绩？不过对于我这种外行来说也算不错了，况且这门课的给分好像就不太好。</p>
<p>&emsp;&emsp;专业课还是照常的走。计组的实验真是和上学期的数逻一样恶心，上学期数逻sqs班的实验简直就是地狱折磨，没想到这学期硬件课逃到了zmm门下，实验居然还是跟着sqs班的课件。Slide上面写的代码没什么章法的贴出来，有些地方甚至sqs还批注：”此代码有误“。又找不到参考代码，也不知从哪里做起，真是折磨。好在其他课程还算进展顺利。</p>
<p>&emsp;&emsp;四月印象当中还是很忙，不过如今回忆起来好像也忙的浑浑噩噩，清明节也没有时间出去玩。算是自己效率低下了吧。</p>
<p>&emsp;&emsp;期中考试都还不错，除了ADS白给了两道，不过影响不大。</p>
<h2 id="科研"><a href="#科研" class="headerlink" title="科研"></a>科研</h2><p>&emsp;&emsp;科研也算正式开始了。SRTP答辩之后申请到了省创，还是觉得可惜。杨洋老师给我分配的项目是关于AI制药的，我和我的小队选择了做分子的设计和优化的方向。答辩准备的很充足，项目也很好，难度也很大，本以为可以申请到国创。不过好在除了名号和经费的区别，实际上并没有什么太大的区别。</p>
<p>&emsp;&emsp;虽然已经下定决心希望能通过这次项目发一篇paper了，但是四月的懈怠导致项目迟迟没有开始push。YY问我项目开始做没有，自己也有点不好意思，搪塞了几句。必须要开始花大功夫开始做科研了，毕竟如果能搞出不错的结果，对自己意义非凡。同专业的人都太强了，我现在还是远远不够。</p>
<p>&emsp;&emsp;从长远来看，身边人的水平的确是在逐步提高，所以侧面说明自己在不断进步？小学初中拿第一较为简单，高中单科倒是拿过挺多次第一，综合只拿过一次。到了大学拿第一就很难了，同专业周围人都很刻苦，也很聪明，自己平常又爱偷懒，自然拉开了较大的差距。革命尚未成功，同志仍需努力。</p>
<p>&emsp;&emsp;暑假多半是不会回去了。本来估摸着是要和好朋友们去哪玩玩，如今看起来确实难。把这个暑假花在科研上面对我来说能提升蛮多的。</p>
<p>&emsp;&emsp;少说多做，少说多做。</p>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p>&emsp;&emsp;实习结束了，互娱这边内推了90多个，雷火那边60多个，数据已经非常不错了，应该在所有大使里面能排进前10？互娱的基本工资在5月中旬发放了，还算不错。打算拿这些钱买个拍立得，再给外婆和婆婆买件衣服，剩下的余钱就先收着。</p>
<p>&emsp;&emsp;四月除了课程实习科研，偶尔还是有空闲的时间。平常在看创造营，和《灵笼》、《窥探》。庆怜没有出道是没想到的。虽然时隔超男超女这么多年看选秀，从微博的舆论氛围和每次的排名还是能闻到现在饭圈和资本的异味，真的很畸形。这样的培养体系是真的“出道即巅峰”，因为大部分人的巅峰就在出道的那一晚上了hh。其他没有出道的倒像是被资本吸干血就扔掉了。饭圈也是混乱，每次排名一出真是腥风血雨。已经很难理解现在饭圈女孩的心理。看到青你魏宏宇和余景天的一些粉丝乱象，真的感到恶心。国家整治饭圈的确不无道理。</p>
<p>&emsp;&emsp;《灵笼》真是国漫之光，各种意义上的。希望灵笼能走向世界，这是会让他国文化的人也会深受感触的作品。</p>
<p>&emsp;&emsp;Rng时隔三年重夺春冠，老粉看到Gala五杀真的热泪盈眶（夸张，并没有）。希望在msi有好的成绩，最好夺冠，重振Rng荣耀。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>&emsp;&emsp;四月算是平庸的一个月，不过过的也算比较舒服。总结来说，虽然时间基本都拿来work了，不过因为懈怠导致时间利用率很低。希望五月可以抓紧时间，这样多抽点时间来搞科研，偶尔也可以写写一些有意思的文章。从五月开始，争取更新一些有意思的文章和技术博客到网站上。</p>
<p>&emsp;&emsp;五月加油~</p>]]></content>
      <categories>
        <category>Life</category>
        <category>Summary</category>
      </categories>
      <tags>
        <tag>Life chips</tag>
        <tag>Summary</tag>
      </tags>
  </entry>
  <entry>
    <title>04\07 生活随笔</title>
    <url>/2021/04/07/04-07-%E7%94%9F%E6%B4%BB%E9%9A%8F%E7%AC%94/</url>
    <content><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="爷的心，禁止访问.jpg" data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <script id="hbeData" type="hbeData" data-hmacdigest="9ca49fa81a2eeb3e6fb809025f474a8f345f0b3cef835549bb454a5c5c1c4e63">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</script>
  <div class="hbe hbe-content">
    <div class="hbe hbe-input hbe-input-default">
      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">
      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">
        <span class="hbe hbe-input-label-content hbe-input-label-content-default">爱因斯坦说他猜出来了</span>
      </label>
    </div>
  </div>
</div>
<script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>Life</category>
        <category>Essay</category>
      </categories>
      <tags>
        <tag>Life chips</tag>
        <tag>Summary</tag>
        <tag>Classified</tag>
      </tags>
  </entry>
  <entry>
    <title>一口会说话的锅</title>
    <url>/2021/03/28/%E4%B8%80%E5%8F%A3%E4%BC%9A%E8%AF%B4%E8%AF%9D%E7%9A%84%E9%94%85/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>&emsp;&emsp;从前有一口锅。</p>
<p>&emsp;&emsp;锅不是普通的锅，是城里最棒的铁匠，用最上乘的纯铁，花最长的时间打造出来的最好的锅。</p>
<p>&emsp;&emsp;锅不是别人的锅，是铁匠的女儿小玉的锅。</p>
<p>&emsp;&emsp;小玉用锅来做烙饼。每每小玉做烙饼的时候，香气总会沿着门缝溢出房门外，循着香气来到门前的人也就越来越多。小玉就笑着把门打开，把做好的烙饼分给大家。等到她自己能吃上的时候，不知道做了多少轮烙饼了。</p>
<p>&emsp;&emsp;锅很幸福。锅不想做别的，只想静静地为小玉烙好一个又一个的饼子。</p>
<p>&emsp;&emsp;小玉正值十七，是出嫁的妙龄。有一天，一个公子带了一大帮人来，带走了小玉。房子里只留下了捂着脸流泪的铁匠，几个还未煎好的烙饼，和孤零零的锅。锅突然开口说话了：</p>
<p>&emsp;&emsp;”小玉——“</p>
<p>&emsp;&emsp;铁匠吓了一跳。锅从灶上蹦了下来，开始扑通扑通，一蹦一蹦朝着门外跳去。</p>
<p>&emsp;&emsp;”我要找到小玉！“</p>
<p>&emsp;&emsp;从此，锅离开了铁匠的家，踏上了寻找小玉的征程。</p>
<br>

<p>&emsp;&emsp;锅坚定无畏地向前行进着。不知道过了多久，走过了多少地方，但始终没有看到小玉的身影。</p>
<p>&emsp;&emsp;锅在路上遇上了一个文官。文官把他拿起来打量了一番：“多好的锅！应该拿来熬树皮做纸。”</p>
<p>&emsp;&emsp;锅讨厌做纸。锅挣脱开了文官的手，吭哧落了地。</p>
<p>&emsp;&emsp;”我要找到小玉，我要找到小玉。。。“</p>
<br>

<p>&emsp;&emsp;锅坚定无畏地向前行进着。不知道过了多久，走过了多少地方，但始终没有看到小玉的身影。</p>
<p>&emsp;&emsp;锅离开陆地，漂流在海上，遇到了一艘船。一个戴着帽子，棕色头发的航海家把他捞了起来：“多好的锅！应该拿来做菜，这样我就能撑住，直到找到新大陆了。”</p>
<p>&emsp;&emsp;锅讨厌做菜。锅挣脱开了航海家的手，扑通跳下海。</p>
<p>&emsp;&emsp;”我要找到小玉，我要找到小玉。。。“</p>
<br>

<p>&emsp;&emsp;锅坚定无畏地向前行进着。不知道过了多久，走过了多少地方，但始终没有看到小玉的身影。</p>
<p>&emsp;&emsp;锅遇上了一支军队，被一个骑在马上的矮矮的将军抓了去。”多好的锅！这是我的战利品。“将军傲慢的说。</p>
<p>&emsp;&emsp;锅讨厌被当作战利品。锅生气了，身体开始变得又红又热，将军被烫的松了手。锅蹦到了地上，气冲冲地离开了。</p>
<p>&emsp;&emsp;”我要找到小玉，我要找到小玉。。。“</p>
<br>

<p>&emsp;&emsp;锅有一次遇上了一个少女。</p>
<p>&emsp;&emsp;”我要找到小玉，我要找到小玉。。。“</p>
<p>&emsp;&emsp;锅没空搭理少女，继续往前行进着。</p>
<p>&emsp;&emsp;少女感到自己被忽视了。她向锅喊道：</p>
<p>&emsp;&emsp;”喂！我就是小玉！“</p>
<p>&emsp;&emsp;锅停了下来，回过头来看向少女。</p>
<p>&emsp;&emsp;“小玉有一双像井水一样的眼睛。”</p>
<p>&emsp;&emsp;少女眨了眨自己的大眼睛。</p>
<p>&emsp;&emsp;“小玉有一头像瀑布一样的黑发。”</p>
<p>&emsp;&emsp;少女取下了自己的发簪。</p>
<p>&emsp;&emsp;“小玉有一颗像月亮一样的纯洁的心。”</p>
<p>&emsp;&emsp;少女轻蔑地笑了笑。</p>
<p>&emsp;&emsp;“你不是小玉。你什么也没有。”</p>
<p>&emsp;&emsp;锅讨厌虚伪的少女。他摇了摇头，头也不回地离开了。</p>
<br>

<p>&emsp;&emsp;“小玉就在世上的某一处。只要我挨个挨个找，一定能找到的。。。”</p>
<br>

<p>&emsp;&emsp;锅遇见过草原上的长颈鹿，冰川中的北极熊，雨林里的猴子，海岸边的海鸥。无论刮风下雨、冰雹打雷，锅始终坚定无畏地寻找着小玉。</p>
<p>&emsp;&emsp;周边的房屋越变越高，路上多了很多会奔跑的铁块。锅开始感到有点累了，原来锅不再是小锅，是口老锅啦。</p>
<p>&emsp;&emsp;”小玉。。。“</p>
<p>&emsp;&emsp;锅怕自己撑不到找到小玉的那一天了。锅底开始泛蓝，是忧郁的蓝。</p>
<p>&emsp;&emsp;锅在伤心。</p>
<br>

<p>&emsp;&emsp;锅突然闻到了一股熟悉的香气。他一蹦一蹦，循着香味拐进了一个小巷，找到了一个破烂的小屋。小屋的窗前聚了很多人。锅在人群的缝隙间钻来钻去，钻到了人群的最前面。</p>
<p>&emsp;&emsp;原来是一个老妇在屋子里做烙饼。老人的眼角挤着皱纹，头发花白，安静地用一口烂锅做着烙饼。</p>
<p>&emsp;&emsp;”小玉！小玉！“锅惊喜地跳了起来。屋前的人群被这口会说话的锅吓的赶紧逃走。</p>
<p>&emsp;&emsp;锅用头咚咚的撞着屋门，呼唤着小玉的名字。但是老人仍然静静地做着烙饼。</p>
<p>&emsp;&emsp;原来老人是个聋子。</p>
<p>&emsp;&emsp;锅在门外等了一宿。第二天清晨，等老人打开屋门，才看见了锅。</p>
<br>

<p>&emsp;&emsp;老人开始用锅来代替原来的烂锅做饼。每每老人做烙饼的时候，香气总会沿着窗缝溢出窗外，循着香气来到屋前的人也就越来越多。老人就笑着把窗户打开，把做好的烙饼分给大家。等到她自己能吃上的时候，不知道做了多少轮烙饼了。</p>
<p>&emsp;&emsp;锅很幸福。锅不想做别的，只想静静地为老人烙好一个又一个的饼子。</p>
<p>&emsp;&emsp;老人平常出门捡破烂卖钱，卖的钱买面团回来做饼。老人累了，会坐在锅旁边的椅子上休息。锅就开始滔滔不绝地讲他一路上有趣的经历：讲那个做纸的文官，讲那个傲慢的将军；讲草原与长颈鹿，讲海岸与海鸥。。。阳光照进小屋，老人的眼睛闭起来，规律地呼吸着。</p>
<p>&emsp;&emsp;日复一日，故事讲完了就又从头开始讲起，锅乐此不疲。</p>
<p>&emsp;&emsp;有一天，老人累了，靠在椅子上休息。眼睛闭了起来，却再也没有睁开。小屋门口渐渐聚集了越来越多的人，他们带走了老人。锅感到有点不安。过了几天，有人敲开了屋门，放了一个小木盒子在桌上。于是屋子里只剩下一个小木盒子，几个还未煎好的烙饼，和孤零零的锅。</p>
<p>&emsp;&emsp;锅慢慢意识到了什么。他的身体开始变蓝，蓝的越来越深邃。</p>
<p>&emsp;&emsp;锅变得很冷，比他去过的冰川还要冷。</p>
<br>

<p>&emsp;&emsp;夜里，锅突然开始越来越烫，烧的通红，照亮了黑暗的小屋。天逐渐亮起来，锅的身体却慢慢变暗，成了墨一样的黑。原本还未煎好的烙饼重新滋滋作响，香气溢出晨光透射的窗缝。</p>
<p>&emsp;&emsp;”够啦。“</p>
<p>&emsp;&emsp;从此，锅再也没有开口说过话。</p>]]></content>
      <categories>
        <category>Literature</category>
        <category>Tale</category>
      </categories>
      <tags>
        <tag>Fairy Tale</tag>
        <tag>Imaginative</tag>
      </tags>
  </entry>
  <entry>
    <title>你好，李焕英</title>
    <url>/2021/03/19/%E4%BD%A0%E5%A5%BD%EF%BC%8C%E6%9D%8E%E7%84%95%E8%8B%B1/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><div id="aplayer-jhaqMLMf" class="aplayer aplayer-tag-marker meting-tag-marker" data-id="1822114811" data-server="netease" data-type="song" data-mode="circulation" data-autoplay="false" data-mutex="true" data-listmaxheight="340px" data-preload="auto" data-theme="#ad7a86"></div>



<br>

<br>

<p>这是第一次在电影院里流泪，还不是轻轻地，是有点不能自已。以前老是被吐槽“铁心”。上次让我觉得备受触动的电影是高二时看的COCO，但也只是鼻头一酸。</p>
<br>

<p>这部电影打动我的第一个地方在于，她很真诚。当然，市面上很多电影都可以说自己很“真诚”，但我觉得他们往往只做到了真诚的“诚”，但是不够“真”。光有诚意是不够的。虽然说艺术源于生活而高于生活，但是能够引起观众共鸣的部分一定是和生活强关联的，同时出自于“真心”的。这部电影根据贾玲的母亲改编，我觉得她就很好地把自己想要表达的东西传递给观众了。没有什么高超的拍摄技巧，也许第一次执导还有些青涩，但就是通过简单的情节讲述了一个并不复杂的故事，然后打动了观众。看到电影最后贾玲带着她妈开敞篷跑车兜风，然而镜头一转，车上实际上只有她一个人，这时候我相信每位观众都能明白她内心真实的感受。</p>
<p>第二个就是，电影细节很好，并且很容易引起观众的共鸣。一部定位是“感人”的电影，你不能引起足够的共鸣，那么无论做什么都是没用的，顶多是主创团队自己感动自己，然后让观众知道“嗯，这里[应该]是感人的”，但其实观众并没有被感动到。对我来说，《送你一朵小红花》就是一个类似的例子。但是《你好，李焕英》里面每个细节都是非常容易打动人的，比如说我被打动的一些点：</p>
<blockquote>
<p>台词细节：</p>
<p>“我宝！”</p>
</blockquote>
<p>这句话是贾玲从天上飞下来，李焕英看到她脱口而出的一句话，然后说的同时冲过去抱住了贾玲。这句话只有两个字是真的好。“我宝”这两个字首先是非常地生活化，你换成“乖女儿！”“玲儿！”“危险！”等等任何其他词都会显得很蠢。这里就是要一个很顺口、很生活化的词，才能达到那种效果。并且说这个词的同时，张小斐双手呈张开状，脸上写满了担心，再配上这句话，真的是一下子就戳中人心。像这种类似的台词还有挺多：</p>
<blockquote>
<p>“你干什么来了？”</p>
<p>“让你开心来了！”</p>
<p>“真的呀！”</p>
<p>“对啊，你开心吗(* ^ _ ^ *)”</p>
<p>“我开心呐！”</p>
<p>“是吗？我还能让你更开心！”</p>
</blockquote>
<p>（台词记不太住了大概是这个）这段话其实也是挺朴实无华的，但是配合上演员的演绎，就简单的很触动人心，特别是当观众知道，其实说这句话的时候，李焕英也是穿越过来了的，这段简单的话就更值得回味了。细节十足。</p>
<p>除开台词，动作或者场景的细节也是蛮多的。</p>
<blockquote>
<p>“妈，我考上省艺校了！”</p>
</blockquote>
<p>当贾玲说这句话的时候，李焕英（老态）手里拿的面团，她听罢兴奋的拍手，面团上的面粉一下子在空中飞舞。并且这个时候阳光刚好从斜方照在李焕英的脸上，皱纹和飞起来的粉灰都泛着光。这一小段真的是特别好，一下子就把母女俩生活中的小美好表现了出来，非常贴近真实。而且这一小段放在结尾高潮煽情处，是经典的以乐景衬哀情，观众看到这一幕很难不被打动。还有一个类似的场景是李焕英在大雪天送贾玲离开，嘴上答应好她坐车回去，实际上为了省钱又不让贾玲担心，等贾玲离开后就自己一个人从车上下来，顶着风雪，走路回去了。这些动作、台词或者场景都很简单，但就是有着击中人心的力量。</p>
<p>再然后就是情节的设计。</p>
<p>情节的设计简直可以和《唐人街探案3》作正反面教材。唐探3一会儿又是寻找凶手，一会儿又是长泽雅美被绑架，剧情就变成了完成K给的任务，再然后和K进行了一堆狗屁不通的对话就去法院说找到凶手了。逻辑并不连贯。但是《你好，李焕英》的情节很清晰。穿越前就说了关于厂里的第一台电视和排球的事，然后穿越过去先是赶上了买电视，大家看完电视后厂长趁着高兴说起了排球比赛的事，这时候贾玲想起穿越前排球冠军有“好事”，就积极和李焕英准备比赛；准备完比赛发现好事是和沈光林相遇，贾玲觉得自己是累赘所以想让李焕英和沈光林在一起…… 总之，情节是连贯的，并且转折都非常丝滑。并且全篇其实也就主要讲了两件事，一个是排球比赛，一个是撮合沈光林和她妈，但电影就把这两件事讲得很好，也就足够了。</p>
<p>情节不光简单，也很“合适”。印象深的是，开头贾玲作假被发现，全场瞬间安静的那段。按照正常的发展，这里应该会产生冲突，贾玲被李焕英痛斥一顿，再穿越过去悔悟。比如说《夏洛特烦恼》开头就是这样的展开。夏洛在秋雅婚礼上充大被马冬梅戳穿，产生“矛盾”，然后夏洛回到过去，才逐渐了解到马冬梅的好。但贾玲并没有选择这么设计。假证被发现后，画面一黑一转，李焕英载着贾玲回家。这里真的远超乎我意料。真的非常不喜欢假证被发现之后“理应”继续的情节，因为那个场面会很尴尬，让人看了也非常尴尬。这段让我的观感和好感一下子就提起来了。并且，李焕英载贾玲回去也并没有责备她，反而笑着“吐槽”女儿，也很真实的说“不用赚那么钱”。在贾玲发现妈妈没有生气之后，开始在自行车后座吹嘘自己的未来，李焕英只是笑着，和女儿一起说着美好的将来。这里也“一下子”就把李焕英的人物形象刻画出来了，不按常理出牌的设计刻画出了李焕英温柔慈祥的形象。</p>
<p>当然，还有最后“原来李焕英也穿越到了过去”的设计，真的是传神之笔，也让人有了二刷的意愿。没什么好说的，就是单纯的太厉害了。</p>
<p>打动我的最后一个点，就是人物本身的刻画。李焕英，真的是一位非常伟大的母亲，并且电影非常生动的描画出了这个形象。我想，这部电影之所以一定要贾玲来执导的原因，是因为只有她才知道，她的母亲在怎样的情形下，到底会说什么样的话，做什么样的动作，有什么样的神态。其余配角的刻画也很不错。</p>
<br>

<p>《你好，李焕英》的英文名是”Hi, Mom“。记者问贾玲，为什么中文名不也叫”你好，妈妈“。贾玲的回答是”妈妈首先是她自己，再是我的妈妈。“电影一开头，贾玲说她的印象中母亲一直是苍老的，但穿越过去，才看见了年轻漂亮，朝气活泼的李焕英。</p>
<p>因此我想，电影除了亲情的感动之外，也想告诉我们一个深刻的含义：任何母亲都不仅仅是”母亲“，首先是她”自己“，也曾拥有无忧无虑的童年，光彩夺目的青春。所以母亲除了作为“母亲”应该被儿女所爱，也应该作为一个独立的女性，被我们去尊重，去理解， 去爱。</p>]]></content>
      <categories>
        <category>Comments</category>
        <category>Film</category>
      </categories>
      <tags>
        <tag>Film</tag>
        <tag>Review</tag>
        <tag>Recommend</tag>
      </tags>
  </entry>
  <entry>
    <title>近期电影汇评</title>
    <url>/2021/03/18/%E8%BF%91%E6%9C%9F%E7%94%B5%E5%BD%B1%E6%B1%87%E8%AF%84/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h2 id="送你一朵小红花"><a href="#送你一朵小红花" class="headerlink" title="送你一朵小红花"></a>送你一朵小红花</h2><p>一部70分的电影。70分主要是给配角们的演技，以及拍摄的质量，还有电影的主题。</p>
<p>剩下30分主要是槽点太多了。</p>
<p>最大的槽点是男女主雨中相拥那一段。在电影当中，这里前面一开始两人在争论，而且不是普通的争吵，是一个很严重的社会话题，我作为观众的情绪也应该是往思考和沉重方面靠。结果男住突然来一句”我喜欢你“，然后两个人雨中相拥。现代医患心理深刻问题矛盾一秒变少年雨中青涩爱情桥段。</p>
<p>这段前面两人的争吵也很神奇。女主说服男主的方式居然是随便在门口找了一个寻亲的奶奶和境遇悲惨的送外卖的来比惨。这两个角色前面一点铺垫没有，突然冒出来，这不是铁工具人这是什么。而且大雨天这俩工具人也站在门口，在没有铺垫的情况下会显得不合理和突兀。</p>
<p>不合适的情节还有很多。比如男主匿名给失去女儿的父亲点红烧牛肉盖饭。这个地方解读很容易产生差异和分歧，有一些人会把这里解读为陌生人的温暖，但也会有部分人认为这里是迷惑操作。</p>
<p>再说说主题。</p>
<p>一部两个小时的电影我看了感觉看了得有3个小时。这部电影想表达的主题太多了：医患心理，亲情，爱情，患者之间的帮扶友情… 当然，在患者的生活中，这些元素都是有的。但是电影没有安排一个脉络清晰的逻辑主线，让这些元素显得非常乱，并且也没有一个明显的主次关系。在观影过程中，大概就是让人看到某个情节，然后知道“这里是个泪点”，但也就仅仅知道而已，其实内心毫无波动。同时，密集而没有突出的泪点也容易让观众产生疲劳。全片看下来我唯一觉得做的不错的就是韦一航家庭，父母和儿子关系这一part还可以。</p>
<p>《送你一朵小红花》的想法是好的，诚意也是有的，但是覆盖了很多华而不实的表皮，演员演技也参差不齐，让人很难入戏。</p>
<h2 id="心灵之旅"><a href="#心灵之旅" class="headerlink" title="心灵之旅"></a>心灵之旅</h2><p>皮克斯真是从来没让我失望。</p>
<p>一直觉得皮克斯做的动画是全龄向的童话，这一部更是证明了这一点。动画虽然看起来简单且儿龄向，设定与情节也算比较老套的了，但是即使是成年人看了也会深受感动，因为皮克斯精准而又得当的表达了一个深刻的主题。之前的COCO也是一样。</p>
<p>成人童话真的是非常必要的。实际上每个大人都有属于自己的幻想世界，也许甚至比小朋友的更丰富多彩，但是却越来越隐秘了。现实生活中，人们随着年龄的增长在心灵处处设防，所以很多描述实际的文学影视作品难以感动成年人群。但是童话是一座完全不同于现实作品的桥梁，他能绕开层层的厚墙，直通每个人心中尚在的一片理想花园，轻吟一段神秘的咒语，便能浇灌人们心中干枯的花蕊。</p>
<p>《小王子》就是这样的一部作品，皮克斯的动画也是。</p>
<h2 id="刺杀小说家"><a href="#刺杀小说家" class="headerlink" title="刺杀小说家"></a>刺杀小说家</h2><p>感觉结尾还是有点可惜，有点像一副好牌打得稀烂。不知道是改编的问题还是原作本来结尾就很烂。</p>
<p>整部作品的设定是非常有有意思的，现实与小说的映射，执笔主宰未来的人生。电影部分情节的转场与特效也很漂亮，印象深刻的有两个：一个是路空文在顶楼差点摔落的时候，眼睛看往楼下而转到了另一个世界。这一段两个世界的无缝衔接很棒。还有一个是关宁和小橘子被红衣武士看见，准备追杀时，魁梧的红衣武士背靠夕阳，身披长铠，手执大刀。这个画面让人印象深刻，可能是画面构图以及红衣武士设计的原因。</p>
<p>但是整体主题并不突出，想做反乌托邦但也没能很好地体现，结尾也是有点哭笑不得，感觉有点可惜。不过还是比《唐人街探案》好吧。</p>]]></content>
      <categories>
        <category>Comments</category>
        <category>Film</category>
      </categories>
      <tags>
        <tag>Film</tag>
        <tag>Review</tag>
      </tags>
  </entry>
  <entry>
    <title>三月上</title>
    <url>/2021/03/17/%E4%B8%89%E6%9C%88%E4%B8%8A/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="294" height="86" src="//music.163.com/outchain/player?type=2&id=1472921626&auto=1&height=66">
</iframe>



<h2 id="日常"><a href="#日常" class="headerlink" title="日常"></a>日常</h2><p>深刻意识到大一和大二上欠“债”太多，现在开始要好好调整安排和作息。</p>
<p>买了一个驼色的秋刀鱼TN手账，旅行款的那种，比较小一个，适合随身携带。看了下网上的手账使用方法，发现和自己想象的不是同一个世界。原来做手账也是一门学问，好的手账er可以做的非常精美有意思。不过对于我这种工科男来讲，能用的就是记录一些idea和要做的事情、问题之类的。或者贴一些有意思的东西，比如去年去迪士尼留下来的门票。</p>
<p>每天早上7：20起床，8：00到图书馆进朗读亭开始读英语。上大学之前以为自己的英语口语还是不错的，直到进入广播站英文专题才发现，简直就是xx。开始听VOA跟读，打开电脑看B站上面Lisa的美音练习，练到9：00，再自习一会儿就去上课。晚上十点后开始自学日语，学到十一点上床睡觉。</p>
<p>这学期玩的时间几乎没有了，手机上游戏和B站都卸了，只有偶尔看看Rng的比赛，或者看点别的。最近在看创造营2021，觉得还挺有意思的，现在基本每一期都会看。打算近期写一篇相关文章评价这次创造营2021。</p>
<h2 id="实习"><a href="#实习" class="headerlink" title="实习"></a>实习</h2><p>寒假的时候面试了4次，一次是网易雷火的游戏研发岗。这个面试卡了，一是因为暑期实习时间不够，二是岗位要求比我想象的更多，主要面向大三及以上的学生，和我这种小屁孩儿没什么关系。</p>
<p>第二次是字节跳动的校园大使。面试官上来就是问我运营了什么什么社群，有没有什么组织经历，是学生会的人吗？本着良心起见，我一律摇了摇头。再一问我发现我是大二的，她开始说：“嗯嗯，好的好的，大概情况我了解了。”我心里也明白，她的意思是：”嗯嗯，你可以走了“。</p>
<p>后两次分别是网易雷火和互娱的校园大使面试，经过前一次的惨败我有教训了，开始了<del>不要脸</del>适度的夸大自己。</p>
<blockquote>
<p> 在广播台里任职，传媒资源广               -&gt; 其实只是每周做节目而已，跟传媒一点关系都没有。</p>
</blockquote>
<blockquote>
<p>虽然是大二，但认识非常多的学长       -&gt; 认识两个学长</p>
</blockquote>
<blockquote>
<p>和大二大三的年级辅导员关系都好       -&gt; 和两个辅导员各交流过两句话</p>
</blockquote>
<blockquote>
<p>经常在各大bbs混迹                                -&gt; 在bbs上发过两条卖书帖</p>
</blockquote>
<blockquote>
<p>我将如何宣传网易：xxxxxxxxx            -&gt; 吹牛有一手的</p>
</blockquote>
<blockquote>
<p>交友范围广，涉及各大学院                   -&gt; 呵呵</p>
</blockquote>
<p>总而言之，凭借着自己的真才实学得到了两个校园大使的Offer，后来发现不冲突，正好又都是网易的时候，就干脆一起当了。进去之后才发现，无论是网易还是互娱，我都是唯一一个大二的…..</p>
<p>感觉互娱和雷火好不一样。都是网易游戏旗下事业部，雷火一进去就是HR让你宣传，拿着内推码去拉人，其他什么也没说，文案什么的都得自己想自己编，怎么宣传也是自己的事，所以刚进去一脸懵逼。</p>
<p>互娱刚进去的时候还搞了一个校园大使欢迎会，不同地区（华东，华南……）的组队，做自我介绍，还有xx活动，还要表现评级，第一名的队伍还有网易周边…… 搞得还蛮隆重的。我们那一组就是江浙一代的，都是东南、南大、南邮、南理、浙大。所以给队伍取名字的时候，我想到用地域谐音梗取名，取了一个：</p>
<blockquote>
<p>南道旧浙（难道就这）</p>
</blockquote>
<p>HR看了直呼鬼才，hh。</p>
<p>雷火每天的校园大使群里HR只负责答疑，互娱校园大使群里HR小姐姐感觉和大家都很亲近，并且每天都有新的任务发布，而且有专门的worktile工作系统。怎么拉社群，怎么编辑文案，怎么和大家沟通，互娱HR每天都会有新的相关物料放送。难道就这队里有几位老前辈也会分享相关经验，我还是挺收益匪浅的。</p>
<p>实习的第一个难点就是拉社群。雷火要求社群至少100个，互娱至少200个。对我来说真的是太难了。面向大二我都还好，关键是岗位基本都是面向大三的，我能碰到的大三学生资源太少了。无奈，只有在学校BBS里发帖，然后每天顶好几次帖，保持高曝光率。</p>
<p><img src="/2021/03/17/%E4%B8%89%E6%9C%88%E4%B8%8A/bbs1.png" alt="bbs1"></p>
<p><img src="/2021/03/17/%E4%B8%89%E6%9C%88%E4%B8%8A/bbs2.png" alt="bbs2"></p>
<p>然后再让大学好友高中兄弟们去帮忙转发拉人啦，找辅导员或者学长帮忙转发在大三的社群里啦… 也算是凑齐了人数。</p>
<p>第二个难点就是时刻被打扰。现在还在学习，但是一天到晚就会有很多不同的人来私戳问你各式各样的问题。有意思的是，基本都以为我是网易员工，以为是前辈，无论是大三大四还是硕士，加了我好友第一句都是：</p>
<p>”师哥好！“</p>
<p>一开始我还解释，后来就懒得解释了。问题还是蛮多样的，有的人真的是时时刻刻都充满了忧虑，问一些我都不知道咋回答的问题”我的简历怎么还没筛过啊？“”你这个内推真的有用吗？“”你说我该不该投递这个岗位啊？“”面试官问我xxxx我该怎么办“”我有一个条件不符合怎么办….“</p>
<p>还有的人跟你聊了几句就开始聊人生，讲他最近咋咋咋，什么不顺心啦，然后开始讲起他的过去…… 你要耐心的听完，并给出一些合理的建议。</p>
<p>开始的两周真的是累死。想文案，写文案，每天运营社群，联系各种资源…… </p>
<p>好在势头还不错，全国90多个校园大使排到了第7，互娱HR奖励了一个阴阳师达摩杯。</p>
<p><img src="/2021/03/17/%E4%B8%89%E6%9C%88%E4%B8%8A/%E5%86%85%E6%8E%A8.jpg" alt="内推"></p>
<p>作为一个大二生，还是蛮不容易的hh。不过之后的内推的人越来越少了，能不能在4月中旬（工作结束）前完成指定kpi，还是个未知数。</p>
<h2 id="社会实践"><a href="#社会实践" class="headerlink" title="社会实践"></a>社会实践</h2><p>寒假的时候搞了红色社会实践，一个25个人的团，我是团长。一开始本来想去抱大腿去加别人的团，结果别人的腿都被抱完了，只好伸出了自己瘦瘦的脚杆。结果发现和我一样的人还有很多，于是上了我的贼船。后来这个船越来越大，什么药学院传媒学院外国语学院的都来了，就成了个25个人的团。当时立项申请书还有填申报表基本都是我一个人写的，搞了好久，还蛮累得。</p>
<p>这学期开始把材料啥的都交了，以为就结束了。结果辅导员找到我说，你们这个团搞得不错啊，学院里打算推一个名额去参加挑战杯今年临时开的红色专项，就推你们了！当时是周三晚跟我说的，结果下周一就要交了。比赛要交一个5min短视频和万字调研报告，本来打算就用寒假社会实践做的视频随便挑一个上去，调研报告就从自己写的总结报告里改改，没想到辅导员甩给我一个别的学校做的视频链接，点进去一看，好家伙，怎么做的这么好。下来一查才发现，这挑战杯红色专项老早在寒假就发布了，别的学校很早就开始准备，结果我们学校截止前一周不到发布临时通知，让每个学院推一个名额上去。我一下子懂了我们团是干什么的了，这波是临时被抓上阵的炮灰。</p>
<p>辅导员还和我语重心长的说：”你们团是咱计院的独苗苗啊！加油，我看好你们。“</p>
<br>

<p>学院只让我带9个人去参加比赛。要从24个人当中选9个，我只好客观的选了寒假社会实践参与度高的同学进来，这样大家都尽量服气。另外保了一下寒假做视频的同学，这样好准备这次的短视频。</p>
<p>接下来就是紧急召集团队，策划视频，写文案，找人借摄像机拍一些补充视频素材，最后剪辑加特效、渲染。</p>
<p>准备比赛这几天真的很忙，我感觉我上学期期末都没这么忙。每天凌晨一点睡，早上7点起，组织拍视频，和技术组的同学沟通，还有监督文案的配音。因为我负责了主要的策划，所以每个人在做的时候我都要在旁边不断地交流想法。加上我只让了两个视频剪辑的同学进组，这次剪辑和渲染的时间也特别紧张，还得临时调整方案。</p>
<p>结果到了最后一天，做视频开头的同学的电脑由于配置太低，渲染的时候死机了，没时间做结尾了。然后正好当时又联系不上他，半天不知道他那边是啥情况，发了十条信息，可能隔了三个小时回了我一条不知所云的信息，然后就又断开链接了。当时我还在咖啡店写具体的分镜和调研报告的结尾，一边写一边着急，因为第二天中午就要交稿了。后来实在没办法，向上面请求宽限了一天，动用了钞能力，找淘宝上一家视频代做的做一下结尾，打电话和那个淘宝剪辑师交流，再把我写的分镜给他，最后也还算完成了。</p>
<p>3月16日，材料上交的第二天早上没课，我好好地补了一觉，十点才起的床。</p>
<br>

<p>这次比赛虽然结果还没出来，但我已经挺有收获的。现在都还记得交稿前一天晚上在咖啡店联系不上剪辑的同学而焦急的自己。明明已经很努力了，有一天午饭没来得及吃，有一天晚饭没吃，但最后还是那么狼狈。一直在想，到底是组员的问题，还是我组长的问题，还是说学院这么晚发通知的问题？非常怀疑自己有没有做组长的能力。</p>
<p>后来反思了一下，觉得主要还是自己的问题。当时应该多招一点技术剪辑的同学进来，而且我想做的很好，导致视频剪辑难度变高，很难在这么几天赶出来，无法跟那些从寒假就开始准备的队伍比较。自己决策失误了。人员的调度也有点问题。最后没联系上剪辑同学的时候也不该很焦急，应该冷静分析一下该怎么办，比如先把别的处理好或者联系别的同学寻求帮助，方法还是很多的。</p>
<h2 id="科研"><a href="#科研" class="headerlink" title="科研"></a>科研</h2><p>寒假的时候开始有搞科研的想法，于是于三月初一个星期四的晚上，写好了邮件发给了我心仪的导师：杨洋。</p>
<p>杨洋是我的班主任，不过只有大二上开学见过一次，后面就没见过了，直到这学期他是我的ADS老师。大二上第一次见面的时候以为他是我的同学，只是长得比较成熟，没想到居然是我们的班主任，真是不可思议。身高挺高的，穿着很简单，喜欢笑，笑起来是比较憨厚的那种。但是他说几句俏皮话之后，就会明白这个人应该是属于聪明那一挂的。</p>
<p>后来才知道，他是清华博士，现在是浙大的博导+副教授，人工智能系主任，才30出头，真是年少有为啊。不仅如此，居然都有两个孩子了，真的是把我震撼到了。毕竟第一次见面的时候我还以为他是我同学。</p>
<p>他人感觉很风趣，也很可靠，并且也非常有实力，上ADS的时候感觉也很照顾学生。读了他的一些论文，再去了解他做的领域（dm），考虑再三之后，最终决定好了要发邮件。描述了一下自己过往的一些经历，和自己的一些想法。反复审查几遍没什么问题之后，邮件就发送出去了。</p>
<p>没想到第二天早上就有了回复。</p>
<p><img src="/2021/03/17/%E4%B8%89%E6%9C%88%E4%B8%8A/%E7%A7%91%E7%A0%94.png" alt="科研"></p>
<p>看到回复的时候当时应该是这学期目前最高兴的时候。</p>
<p>后来和同进组的同学交流了一下，发现我进的还是很快很顺利的。可能是他觉得我高中有竞赛基础吧。</p>
<p>加了他微信后，又加了实验室的群，科研之旅也算是正式开始了。已经做好了学一堆新知识看一堆论文的准备了。</p>
<p>PS：杨洋老师的头像和他本人一样的有趣。</p>
<p><img src="/2021/03/17/%E4%B8%89%E6%9C%88%E4%B8%8A/%E6%9D%A8%E6%B4%8B.jpg" alt="杨洋"></p>
<h1 id="三月上"><a href="#三月上" class="headerlink" title="三月上"></a>三月上</h1><p>とにかく，三月上还是很忙的。忙到没有时间也没有心情去看学校里到处开的花儿。广播站有同学趁着周末去了趟武汉赏花，想来还是挺羡慕的。</p>
<p>不过换种方式想想，我也算忙的充实。迈入了最期待的科研大门，希望能开启一段有意思的旅程。</p>
<p>GoGoGo :sunglasses:</p>]]></content>
      <categories>
        <category>Life</category>
        <category>Summary</category>
      </categories>
      <tags>
        <tag>Life chips</tag>
        <tag>Summary</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2021/02/09/hello-world/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>作为第一篇文章，希望这里能成为我喜欢休憩的净土，和一个有趣的世界。</p>]]></content>
      <tags>
        <tag>Beginning[开始]</tag>
      </tags>
  </entry>
</search>
